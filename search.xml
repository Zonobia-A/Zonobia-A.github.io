<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>实战TensorFlow</title>
    <url>/2021/09/20/%E5%AE%9E%E6%88%98TensorFlow/</url>
    <content><![CDATA[<p>二分类逻辑斯蒂回归TensorFlow实战<a id="more"></a></p>
<h3 id="实验环境（Python）"><a href="#实验环境（Python）" class="headerlink" title="实验环境（Python）"></a>实验环境（Python）</h3><ul>
<li>matplotlib</li>
<li>numpy</li>
<li>tensorflow</li>
<li>pickle</li>
<li>os</li>
</ul>
<h3 id="总体结构"><a href="#总体结构" class="headerlink" title="总体结构"></a>总体结构</h3><p><strong>Cifar数据集文件结构：（2-6是训练数据，7是测试数据）</strong></p>
<ol>
<li><em>batches.meta</em></li>
<li><em>data_batch_1</em></li>
<li><em>data_batch_2</em></li>
<li><em>data_batch_3</em></li>
<li><em>data_batch_4</em></li>
<li><em>data_batch_5</em></li>
<li><em>test_batch</em></li>
</ol>
<p><strong>Cifar数据结构：</strong></p>
<ol>
<li><em>data (60000x3042的图像)</em></li>
<li><em>labels (60000x1的图像分类)</em></li>
<li><em>batch_labels (在batch1-5中，没有实质意义)</em></li>
<li><em>filenames (文件名字)</em></li>
</ol>
<h4 id="1-数据读取（可以使用tensorflow-Dataset）"><a href="#1-数据读取（可以使用tensorflow-Dataset）" class="headerlink" title="1.数据读取（可以使用tensorflow.Dataset）"></a>1.数据读取（可以使用tensorflow.Dataset）</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># pickle是python3反序列化压缩数据的包</span></span><br><span class="line"><span class="keyword">import</span> pickle <span class="keyword">as</span> p</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_data</span><span class="params">(filename)</span>:</span></span><br><span class="line">    <span class="string">"""read data from data file."""</span></span><br><span class="line">    <span class="keyword">with</span> open(filename, <span class="string">'rb'</span>) <span class="keyword">as</span> f:</span><br><span class="line">        data = p.load(f, encoding=<span class="string">'latin1'</span>)</span><br><span class="line">        <span class="keyword">return</span> data[<span class="string">'data'</span>], data[<span class="string">'labels'</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Cifar数据类，需要提供的结构就是类似于Dataloader的next_batch功能</span></span><br><span class="line"><span class="comment"># need_shuffle是指数据是否需要进行一次打乱重组，注意shuffle只出现在training阶段</span></span><br><span class="line"><span class="comment"># _name指的是类中的私有成员变量与私有成员函数</span></span><br><span class="line"><span class="comment"># 下面对外只有next_batch函数借口</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CifarData</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, filenames, need_shuffle)</span>:</span></span><br><span class="line">        all_data = []</span><br><span class="line">        all_labels = []</span><br><span class="line">        <span class="comment"># 将batch_1~5所有数据进行合并，并进行过滤，将其中0和1类的数据提取出来</span></span><br><span class="line">        <span class="keyword">for</span> filename <span class="keyword">in</span> filenames:</span><br><span class="line">            data, labels = load_data(filename)</span><br><span class="line">            <span class="keyword">for</span> item, label <span class="keyword">in</span> zip(data, labels):</span><br><span class="line">                <span class="keyword">if</span> label <span class="keyword">in</span> [<span class="number">0</span>, <span class="number">1</span>]:</span><br><span class="line">                    all_data.append(item)</span><br><span class="line">                    all_labels.append(label)</span><br><span class="line">        self._data = np.vstack(all_data)</span><br><span class="line">        <span class="comment"># 这个归一化是必要的，因为[0-255]值非常大，会使sigmoid后的结果分在两极，最终会导致梯度消失现象</span></span><br><span class="line">        self._data = self._data / <span class="number">127.5</span> - <span class="number">1</span></span><br><span class="line">        self._labels = np.hstack(all_labels)</span><br><span class="line">        self._num_examples = self._data.shape[<span class="number">0</span>]</span><br><span class="line">        self._need_shuffle = need_shuffle</span><br><span class="line">        self._indicator = <span class="number">0</span></span><br><span class="line">        <span class="keyword">if</span> self._need_shuffle:</span><br><span class="line">            self._shuffle_data()</span><br><span class="line">    <span class="comment"># np.random.permutation(num)会生成一个随机化的长度为num的列表，通过生成的随机列表对原始data和	# label进行打乱</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_shuffle_data</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="comment"># [0, 1, 2, 3, 4, 5] -&gt; [5, 3, 2, 4, 1, 0]</span></span><br><span class="line">        p = np.random.permutation(self._num_examples)</span><br><span class="line">        self._data = self._data[p]</span><br><span class="line">        self._labels = self._labels[p]</span><br><span class="line">    <span class="comment"># 这是整个cifar类中最重要的函数，私有成员变量_indicator记录当前访问的data位置，如果到达data数组的末	  # 尾，则将数据打乱，将_indicator再次指向data数组的开始</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">next_batch</span><span class="params">(self, batch_size)</span>:</span></span><br><span class="line">        <span class="string">"""return batch_size examples as a batch"""</span></span><br><span class="line">        end_indicator = self._indicator + batch_size</span><br><span class="line">        <span class="keyword">if</span> end_indicator &gt; self._num_examples:</span><br><span class="line">            <span class="keyword">if</span> self._need_shuffle:</span><br><span class="line">                self._shuffle_data()</span><br><span class="line">                self._indicator = <span class="number">0</span></span><br><span class="line">                end_indicator = batch_size</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">raise</span> Exception(<span class="string">"have no more examples"</span>)</span><br><span class="line">        <span class="keyword">if</span> end_indicator &gt; self._num_examples:</span><br><span class="line">            <span class="keyword">raise</span> Exception(<span class="string">"batch_size is larger than all examples"</span>)</span><br><span class="line">        batch_data = self._data[self._indicator: end_indicator]</span><br><span class="line">        batch_labels = self._labels[self._indicator: end_indicator]</span><br><span class="line">        self._indicator = end_indicator</span><br><span class="line">        <span class="keyword">return</span> batch_data, batch_labels</span><br></pre></td></tr></table></figure>

<h4 id="2-网络图（Graph）定义"><a href="#2-网络图（Graph）定义" class="headerlink" title="2.网络图（Graph）定义"></a>2.网络图（Graph）定义</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> os </span><br><span class="line"><span class="comment"># 定义站位符</span></span><br><span class="line">x = tf.placeholder(tf.float32, [<span class="literal">None</span>, <span class="number">3072</span>])</span><br><span class="line">y = tf.placeholder(tf.int64, [<span class="literal">None</span>])</span><br><span class="line"><span class="comment"># 定义网络中所需参数，因为输入数据是[None, 3072]，输出是[1]，所以w的尺寸应是[3072, 1]，用随机正太分布进# # 行初始化；b的尺寸是[1]，用0.0值进行初始化</span></span><br><span class="line">w = tf.get_variable(<span class="string">'w'</span>, [x.get_shape()[<span class="number">-1</span>], <span class="number">1</span>], initializer=tf.random_normal_initializer(<span class="number">0</span>, <span class="number">1</span>))</span><br><span class="line">b = tf.get_variable(<span class="string">'b'</span>, [<span class="number">1</span>], initializer=tf.constant_initializer(<span class="number">0.0</span>))</span><br><span class="line">y_ = tf.matmul(x, w) + b</span><br><span class="line"><span class="comment"># 二分类的末尾要使用sigmoid进行映射</span></span><br><span class="line">p_y_1 = tf.nn.sigmoid(y_)</span><br><span class="line"><span class="comment"># 维度转换为了后面loss相减操作</span></span><br><span class="line">y_reshaped = tf.reshape(y, (<span class="number">-1</span>, <span class="number">1</span>))</span><br><span class="line"><span class="comment"># 由于p_y_1书float类型，因此要对y的值进行类型转换</span></span><br><span class="line">y_reshaped_float = tf.cast(y_reshaped, tf.float32)</span><br><span class="line"><span class="comment"># 均方误差</span></span><br><span class="line">loss = tf.reduce_mean(tf.square(y_reshaped_float - p_y_1))</span><br><span class="line"><span class="comment"># 计算预测值</span></span><br><span class="line">predict = p_y_1 &gt; <span class="number">0.5</span></span><br><span class="line"><span class="comment"># 计算预测真值</span></span><br><span class="line">correct_prediction = tf.equal(tf.cast(predict, tf.int64), y_reshaped)</span><br><span class="line"><span class="comment"># 计算预测准确度</span></span><br><span class="line">accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float64))</span><br><span class="line"><span class="comment"># 定义训练operation</span></span><br><span class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">'train_op'</span>):</span><br><span class="line">    train_op = tf.train.AdamOptimizer(<span class="number">1e-4</span>).minimize(loss)</span><br></pre></td></tr></table></figure>

<h4 id="3-参数定义"><a href="#3-参数定义" class="headerlink" title="3.参数定义"></a>3.参数定义</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">batch_size = <span class="number">20</span></span><br><span class="line">train_steps = <span class="number">100000</span></span><br><span class="line">test_steps = <span class="number">100</span></span><br><span class="line">all_test_acc_val = []</span><br><span class="line">train_filenames = [os.path.join(CIFAR_DIR, <span class="string">'data_batch_%d'</span> % i) <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, <span class="number">6</span>)]</span><br><span class="line">test_filenames = [os.path.join(CIFAR_DIR, <span class="string">'test_batch'</span>)]</span><br><span class="line">train_data = CifarData(train_filenames, <span class="literal">True</span>)</span><br></pre></td></tr></table></figure>

<h4 id="4-数据feed"><a href="#4-数据feed" class="headerlink" title="4.数据feed"></a>4.数据feed</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 全局参数初始化</span></span><br><span class="line">init = tf.global_variables_initializer()</span><br><span class="line"><span class="comment"># 定义会话</span></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(init)</span><br><span class="line">    <span class="comment"># 训练网络参数</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(train_steps):</span><br><span class="line">        batch_data, batch_labels = train_data.next_batch(batch_size)</span><br><span class="line">        loss_val, accu_val, _ = sess.run(</span><br><span class="line">            [loss, accuracy, train_op], </span><br><span class="line">            feed_dict=&#123;</span><br><span class="line">                x:batch_data, </span><br><span class="line">                y:batch_labels&#125;)</span><br><span class="line">        <span class="keyword">if</span> (i+<span class="number">1</span>) % <span class="number">500</span> == <span class="number">0</span>:</span><br><span class="line">            print(<span class="string">'[Train] Step: %d, loss: %4.5f, acc: %4.5f'</span> % (i+<span class="number">1</span>, loss_val, accu_val))</span><br><span class="line">        <span class="keyword">if</span> (i+<span class="number">1</span>) % <span class="number">5000</span> == <span class="number">0</span>:</span><br><span class="line">            test_data = CifarData(test_filenames, <span class="literal">False</span>)</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> range(test_steps):</span><br><span class="line">                test_batch_data, test_batch_labels = test_data.next_batch(batch_size)</span><br><span class="line">                test_acc_val = sess.run([accuracy], feed_dict=&#123;x:test_batch_data, y:test_batch_labels&#125;)</span><br><span class="line">                all_test_acc_val.append(test_acc_val)</span><br><span class="line">            test_acc = np.mean(all_test_acc_val)</span><br><span class="line">            print(<span class="string">'[Test] Step: %d, acc: %4.5f'</span> % (i+<span class="number">1</span>, test_acc))</span><br></pre></td></tr></table></figure>

<h4 id="5-结果展示："><a href="#5-结果展示：" class="headerlink" title="5.结果展示："></a>5.结果展示：</h4><p><img src="https://i.loli.net/2021/09/21/8a1bOMdnNuiI4so.png" alt="二分类逻辑斯蒂回归.jpg"></p>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><ol>
<li><strong>读取数据部分需要投入很大精力再去深入研究，上述只是实现了最基本的next_batch功能；</strong></li>
<li><strong>网络模型部分重点还是要关注模型调参，超参数定义；</strong></li>
<li><u><strong>后续更新多分类逻辑斯蒂回归TensorFlow实战。</strong></u></li>
</ol>
]]></content>
      <categories>
        <category>深度学习</category>
        <category>tensorflow</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>tensorflow</tag>
      </tags>
  </entry>
  <entry>
    <title>digit_recognizer</title>
    <url>/2020/12/29/digit-recognizer/</url>
    <content><![CDATA[<p>手写体识别 ｜ 入门</p>
<p>digit-recognizer</p>
<a id="more"></a>

<h3 id="问题描述：一个识别数字0～9的练习赛"><a href="#问题描述：一个识别数字0～9的练习赛" class="headerlink" title="问题描述：一个识别数字0～9的练习赛"></a>问题描述：一个识别数字0～9的练习赛</h3><p><img src="https://i.loli.net/2020/12/29/stPSezJQxm6wUhT.png" alt="e201e1cf791d21082ee034415c5d367.png"></p>
<ul>
<li>Overview中介绍了问题的描述</li>
<li>Data里介绍了train，test数据样本，以及最终结果的提交格式</li>
<li>Notebooks是别人或自己分享的笔记或code</li>
<li>Discussion是别人对于该问题的一些很好的意见和讨论</li>
<li>Leaderboard是排名</li>
<li>Rules是参加本次训练的一个规则</li>
</ul>
<hr>
<h3 id="知识储备"><a href="#知识储备" class="headerlink" title="知识储备"></a>知识储备</h3><ul>
<li>ML算法基础</li>
<li>基本的一些编程语言，如Python</li>
<li>算法库或算法包，如numpy，scipy，scikit-learn，tensorflow</li>
</ul>
<p><strong>下面的介绍只会用到tensorflow和numpy的一些知识，都说是入门了</strong></p>
<h3 id="数据集介绍"><a href="#数据集介绍" class="headerlink" title="数据集介绍"></a>数据集介绍</h3><p>每一个digit对应的数据是一个<code>[784, ]</code>–&gt;<code>[28, 28, 1]</code>的矩阵</p>
<h4 id="train"><a href="#train" class="headerlink" title="train"></a>train</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 类型转换函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">toInt</span><span class="params">(array)</span>:</span></span><br><span class="line">    array = np.mat(array)</span><br><span class="line">    m, n = np.shape(array)</span><br><span class="line">    newArray = np.zeros((m, n))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(m): </span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(n):</span><br><span class="line">            newArray[i, j] = int(array[i, j])</span><br><span class="line">    <span class="keyword">return</span> newArray</span><br><span class="line"></span><br><span class="line"><span class="comment"># 归一化函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">normalizing</span><span class="params">(array)</span>:</span></span><br><span class="line">    m, n = np.shape(array)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(m):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(n):</span><br><span class="line">            <span class="keyword">if</span> array[i, j] != <span class="number">0</span>:</span><br><span class="line">                array[i, j] = <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> array</span><br><span class="line">  </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loadTrainData</span><span class="params">()</span>:</span></span><br><span class="line">    l = []</span><br><span class="line">    <span class="keyword">with</span> open(<span class="string">'./digit_recognizer/train.csv'</span>) <span class="keyword">as</span> f:</span><br><span class="line">        lines = csv.reader(f)</span><br><span class="line">        <span class="keyword">for</span> line <span class="keyword">in</span> lines:</span><br><span class="line">            l.append(line)</span><br><span class="line">    <span class="comment"># 由于第一行是train.cvs的描述，所以舍弃第一行</span></span><br><span class="line">    l.remove(l[<span class="number">0</span>])</span><br><span class="line">    l = np.array(l)</span><br><span class="line">    <span class="comment"># cvs文件中的label都是单一数值，在这里转化为onehot形式（7--&gt;[0 0 0 0 0 0 1 0 0 0]）</span></span><br><span class="line">    label = np.zeros([np.shape(l)[<span class="number">0</span>], <span class="number">10</span>])</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(np.shape(l)[<span class="number">0</span>]):</span><br><span class="line">        label[i, int(l[i, <span class="number">0</span>])] = <span class="number">1</span></span><br><span class="line">    data = l[:,<span class="number">1</span>:]</span><br><span class="line">    <span class="comment"># 获得数字数据及对应的label</span></span><br><span class="line">    <span class="keyword">return</span> normalizing(toInt(data)), toInt(label)</span><br></pre></td></tr></table></figure>

<p>使用<code>loadTrainData()</code>函数可以提取<code>train.csv</code>中的所有样本，有以下几个注意点需要注意：</p>
<ul>
<li>由于第一行是train.cvs的描述，所以舍弃第一行</li>
<li>cvs文件中的<code>label</code>都是单一数值，在这里转化为onehot形式（<code>7--&gt;[0 0 0 0 0 0 1 0 0 0]</code>）</li>
<li><code>train.cvs</code>中的数值提取出来后均是<code>string</code>类型，需要转化为<code>Int</code></li>
<li>训练集中共有42000张digit图像</li>
</ul>
<h4 id="test"><a href="#test" class="headerlink" title="test"></a>test</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loadTestData</span><span class="params">()</span>:</span></span><br><span class="line">    l = []</span><br><span class="line">    <span class="keyword">with</span> open(<span class="string">'./digit_recognizer/test.csv'</span>) <span class="keyword">as</span> f:</span><br><span class="line">        lines = csv.reader(f)</span><br><span class="line">        <span class="keyword">for</span> line <span class="keyword">in</span> lines:</span><br><span class="line">            l.append(line)</span><br><span class="line">    <span class="comment"># 由于第一行是test.cvs的描述，所以舍弃第一行</span></span><br><span class="line">    l.remove(l[<span class="number">0</span>])</span><br><span class="line">    data = np.array(l)</span><br><span class="line">    <span class="keyword">return</span> normalizing(toInt(data))</span><br></pre></td></tr></table></figure>

<p>使用<code>loadTestData()</code>函数可以提取test.csv`中的所有样本，有以下几个注意点需要注意：</p>
<ul>
<li>由于第一行是<code>test.cvs</code>的描述，所以舍弃第一行</li>
<li>训练集中共有28000张digit图像</li>
</ul>
<hr>
<h3 id="CNN模型构建"><a href="#CNN模型构建" class="headerlink" title="CNN模型构建"></a>CNN模型构建</h3><h4 id="基本模块"><a href="#基本模块" class="headerlink" title="基本模块"></a>基本模块</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 卷积</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">conv</span><span class="params">(feature, W, s=<span class="number">1</span>)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> tf.nn.conv2d(feature, W, strides=[<span class="number">1</span>, s, s, <span class="number">1</span>], padding=<span class="string">'SAME'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 激活</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">leakey_relu</span><span class="params">(x, alpha)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> tf.maximum(x, alpha * x)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 卷积核初始化</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">weight_variale</span><span class="params">(shape, name)</span>:</span></span><br><span class="line">    init = tf.truncated_normal(shape, stddev=<span class="number">0.01</span>)</span><br><span class="line">    <span class="keyword">return</span> tf.Variable(init, name=name)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 偏置初始化</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">bias_variable</span><span class="params">(shape, name)</span>:</span></span><br><span class="line">    init = tf.constant(<span class="number">0.01</span>, shape=shape)</span><br><span class="line">    <span class="keyword">return</span> tf.Variable(init, name=name)</span><br></pre></td></tr></table></figure>

<h4 id="模型构建"><a href="#模型构建" class="headerlink" title="模型构建"></a>模型构建</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">model</span><span class="params">(digit)</span>:</span></span><br><span class="line">    <span class="comment"># 第一层卷积核</span></span><br><span class="line">    w1 = weight_variale([<span class="number">3</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">32</span>], name=<span class="string">'w1'</span>)</span><br><span class="line">    b1 = bias_variable([<span class="number">32</span>], name=<span class="string">'b1'</span>)</span><br><span class="line">    c1 = tf.nn.relu(conv(digit, w1) + b1)</span><br><span class="line">    <span class="comment"># 第一层池化层</span></span><br><span class="line">    p1 = tf.nn.max_pool(c1, ksize=[<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span> ,<span class="number">1</span>], strides=[<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>], padding=<span class="string">'SAME'</span>)</span><br><span class="line">    <span class="comment"># 第二层卷积层</span></span><br><span class="line">    w2 = weight_variale([<span class="number">3</span>, <span class="number">3</span>, <span class="number">32</span>, <span class="number">64</span>], name=<span class="string">'w2'</span>)</span><br><span class="line">    b2 = bias_variable([<span class="number">64</span>], name=<span class="string">'b2'</span>)</span><br><span class="line">    c2 = tf.nn.relu(conv(p1, w2) + b2)</span><br><span class="line">    <span class="comment"># 第二层池化层</span></span><br><span class="line">    p2 = tf.nn.max_pool(c2, ksize=[<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>], strides=[<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>], padding=<span class="string">'SAME'</span>)</span><br><span class="line">    <span class="comment"># 将矩阵拉成一个vector</span></span><br><span class="line">    flatten = tf.reshape(p2, [<span class="number">-1</span>, <span class="number">7</span> * <span class="number">7</span> * <span class="number">64</span>])</span><br><span class="line">    <span class="comment"># 第一层全连接层</span></span><br><span class="line">    w_fc1 = weight_variale([<span class="number">3136</span>, <span class="number">1024</span>], name=<span class="string">'fc_w1'</span>)</span><br><span class="line">    b_fc1 = bias_variable([<span class="number">1024</span>], name=<span class="string">'fc_b1'</span>)</span><br><span class="line">    fc1 = tf.matmul(flatten, w_fc1) + b_fc1</span><br><span class="line">    fc1 = tf.nn.relu(fc1)</span><br><span class="line"> 		<span class="comment"># 第二层全连接层</span></span><br><span class="line">    w_fc2 = weight_variale([<span class="number">1024</span>, <span class="number">10</span>], name=<span class="string">'fc_w2'</span>)</span><br><span class="line">    b_fc2 = bias_variable([<span class="number">10</span>], name=<span class="string">'fc_b2'</span>)</span><br><span class="line">    fc2 = tf.matmul(fc1, w_fc2) + b_fc2</span><br><span class="line">    fc2 = tf.nn.softmax(fc2)</span><br><span class="line">    <span class="keyword">return</span> fc2</span><br></pre></td></tr></table></figure>

<h4 id="模型训练"><a href="#模型训练" class="headerlink" title="模型训练"></a>模型训练</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">()</span>:</span></span><br><span class="line">    batch_size = <span class="number">3000</span></span><br><span class="line">    epochs = <span class="number">300</span></span><br><span class="line">    <span class="comment"># 占位符</span></span><br><span class="line">    digit_ = tf.placeholder(tf.float32, shape=[<span class="literal">None</span>, width * height * channel])</span><br><span class="line">    digit = tf.reshape(digit_, shape=[<span class="number">-1</span>, width, height, channel])</span><br><span class="line">    y_ = tf.placeholder(tf.float32, shape=[<span class="literal">None</span>, out_class])</span><br><span class="line">    <span class="comment"># 模型预测</span></span><br><span class="line">    pred = model(digit)</span><br><span class="line">    <span class="comment"># 交叉熵损失函数</span></span><br><span class="line">    cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(pred), reduction_indices=[<span class="number">1</span>]))</span><br><span class="line">    <span class="comment"># 优化器</span></span><br><span class="line">    train_step = tf.train.AdamOptimizer(<span class="number">1e-4</span>).minimize(cross_entropy)</span><br><span class="line">    correct_prediction = tf.equal(tf.argmax(pred, <span class="number">1</span>), tf.argmax(y_, <span class="number">1</span>))</span><br><span class="line">    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))</span><br><span class="line">    saver = tf.train.Saver()</span><br><span class="line">    </span><br><span class="line">    train_data, label = loadTrainData()</span><br><span class="line">    train_size = np.shape(train_data)[<span class="number">0</span>]</span><br><span class="line">    max_batch = (train_size<span class="number">-5000</span>) // batch_size</span><br><span class="line">    best_accuracy = <span class="number">0</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">        sess.run(tf.global_variables_initializer())</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(epochs):</span><br><span class="line">            <span class="keyword">for</span> idx <span class="keyword">in</span> range(max_batch):</span><br><span class="line">              <span class="comment"># 加载一个batchsize的数据进行模型训练</span></span><br><span class="line">                batch_data = train_data[idx * batch_size : (idx+<span class="number">1</span>) * batch_size, :]</span><br><span class="line">                batch_label = label[idx * batch_size : (idx+<span class="number">1</span>) * batch_size, :]</span><br><span class="line">                _ = sess.run(train_step, feed_dict=&#123;digit_: batch_data, y_: batch_label&#125;)</span><br><span class="line">            <span class="comment"># 验证集确定模型</span></span><br><span class="line">            val_data = train_data[<span class="number">-5000</span>: , :]</span><br><span class="line">            val_label = label[<span class="number">-5000</span> : , :]</span><br><span class="line">            pre_accuracy = sess.run(accuracy, feed_dict=&#123;digit_: val_data, y_: val_label&#125;)</span><br><span class="line">            print(i, pre_accuracy)</span><br><span class="line">            <span class="keyword">if</span> pre_accuracy &gt; best_accuracy:</span><br><span class="line">                saver.save(sess, <span class="string">'./digit_recognizer/model/digit_recognization.ckpt'</span>)</span><br></pre></td></tr></table></figure>

<h4 id="模型测试"><a href="#模型测试" class="headerlink" title="模型测试"></a>模型测试</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">saveResults</span><span class="params">(result)</span>:</span></span><br><span class="line">    <span class="keyword">with</span> open(<span class="string">'./digit_recognizer/result.csv'</span>, <span class="string">'w'</span>) <span class="keyword">as</span> f:</span><br><span class="line">        writer = csv.writer(f)</span><br><span class="line">        <span class="keyword">for</span> index,i <span class="keyword">in</span> enumerate(result):</span><br><span class="line">            tmp = [index+<span class="number">1</span>]</span><br><span class="line">            tmp.append(i)</span><br><span class="line">            writer.writerow(tmp)</span><br><span class="line">            </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test</span><span class="params">()</span>:</span></span><br><span class="line">    test_data = loadTestData()</span><br><span class="line">    digit_ = tf.placeholder(tf.float32, shape=[<span class="literal">None</span>, width * height * channel])</span><br><span class="line">    digit = tf.reshape(digit_, shape=[<span class="number">-1</span>, width, height, channel])</span><br><span class="line">    pred = model(digit)</span><br><span class="line">    saver = tf.train.Saver()</span><br><span class="line">    <span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">        saver.restore(sess, <span class="string">'./digit_recognizer/model/digit_recognization.ckpt'</span>)</span><br><span class="line">        result = sess.run(pred, feed_dict=&#123;digit_: test_data&#125;)</span><br><span class="line">        result = np.argmax(result, axis=<span class="number">1</span>)</span><br><span class="line">    saveResults(result)</span><br></pre></td></tr></table></figure>

<p><em>未完待续：后续会进一步进行特征工程，可视化，数据扩充。。。</em></p>
]]></content>
      <categories>
        <category>kaggle</category>
        <category>CNN</category>
      </categories>
      <tags>
        <tag>kaggle</tag>
      </tags>
  </entry>
  <entry>
    <title>leetcode丑数</title>
    <url>/2020/05/31/leetcode%E4%B8%91%E6%95%B0/</url>
    <content><![CDATA[<p>丑数</p>
<ul>
<li>第 k 个数</li>
</ul>
<a id="more"></a>

<p>持续更新中。。。</p>
]]></content>
      <categories>
        <category>LeetCode</category>
        <category>丑数</category>
      </categories>
      <tags>
        <tag>LeetCode</tag>
      </tags>
  </entry>
  <entry>
    <title>leetcode二分查找</title>
    <url>/2020/05/31/leetcode%E4%BA%8C%E5%88%86%E6%9F%A5%E6%89%BE/</url>
    <content><![CDATA[<p>二分查找</p>
<ul>
<li>寻找重复数</li>
</ul>
<a id="more"></a>

<p>持续更新中。。。</p>
]]></content>
      <categories>
        <category>LeetCode</category>
        <category>二分查找</category>
      </categories>
      <tags>
        <tag>LeetCode</tag>
      </tags>
  </entry>
  <entry>
    <title>leetcode链表</title>
    <url>/2020/05/31/leetcode%E9%93%BE%E8%A1%A8/</url>
    <content><![CDATA[<p>链表</p>
<ul>
<li>奇偶链表</li>
</ul>
<a id="more"></a>

<h4 id="奇偶链表"><a href="#奇偶链表" class="headerlink" title="奇偶链表"></a>奇偶链表</h4><h5 id="题目"><a href="#题目" class="headerlink" title="题目"></a><a href="https://leetcode-cn.com/problems/odd-even-linked-list/" target="_blank" rel="noopener">题目</a></h5><p>给定一个单链表，把所有的奇数节点和偶数节点分别排在一起。请注意，这里的奇数节点和偶数节点指的是节点编号的奇偶性，而不是节点的值的奇偶性。</p>
<p>请尝试使用原地算法完成。你的算法的空间复杂度应为 O(1)，时间复杂度应为 O(nodes)，nodes 为节点总数。</p>
<p><strong>示例</strong></p>
<figure class="highlight tex"><table><tr><td class="code"><pre><span class="line">输入: 1-&gt;2-&gt;3-&gt;4-&gt;5-&gt;NULL</span><br><span class="line">输出: 1-&gt;3-&gt;5-&gt;2-&gt;4-&gt;NULL</span><br><span class="line"></span><br><span class="line">输入: 2-&gt;1-&gt;3-&gt;5-&gt;6-&gt;4-&gt;7-&gt;NULL </span><br><span class="line">输出: 2-&gt;3-&gt;6-&gt;7-&gt;1-&gt;5-&gt;4-&gt;NULL</span><br></pre></td></tr></table></figure>

<h5 id="题解"><a href="#题解" class="headerlink" title="题解"></a>题解</h5><p>思路：将奇节点放在一个链表里，偶链表放在另一个链表里。然后把偶链表接在奇链表的尾部。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">oddEvenList</span><span class="params">(self, head: ListNode)</span> -&gt; ListNode:</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> head:</span><br><span class="line">            <span class="keyword">return</span></span><br><span class="line">        cur = head</span><br><span class="line">        odd = ListNode(<span class="literal">None</span>)</span><br><span class="line">        cur_odd = odd</span><br><span class="line">        eve = ListNode(<span class="literal">None</span>)</span><br><span class="line">        cur_eve = eve</span><br><span class="line">        <span class="keyword">while</span> cur:</span><br><span class="line">            node = ListNode(cur.val)</span><br><span class="line">            cur_odd.next = node</span><br><span class="line">            cur_odd = cur_odd.next</span><br><span class="line">            cur = cur.next</span><br><span class="line">            <span class="keyword">if</span> cur:</span><br><span class="line">                node = ListNode(cur.val)</span><br><span class="line">                cur_eve.next = node</span><br><span class="line">                cur_eve = cur_eve.next</span><br><span class="line">                cur = cur.next</span><br><span class="line">        cur_odd.next = eve.next</span><br><span class="line">        <span class="keyword">return</span> odd.next</span><br></pre></td></tr></table></figure>

<p>时间复杂度：O(N)</p>
<p>空间复杂度：O(1)</p>
<hr>
<p>未完待续。。。</p>
]]></content>
      <categories>
        <category>LeetCode</category>
        <category>链表</category>
      </categories>
      <tags>
        <tag>LeetCode</tag>
      </tags>
  </entry>
  <entry>
    <title>leetcode前缀和</title>
    <url>/2020/05/31/leetcode%E5%89%8D%E7%BC%80%E5%92%8C/</url>
    <content><![CDATA[<p>前缀和</p>
<ul>
<li>和可被 K 整除的子数组（在哈希专题可以找到）</li>
<li>大小为 K 且平均值大于等于阈值的子数组数目</li>
</ul>
<a id="more"></a>

<h4 id="大小为-K-且平均值大于等于阈值的子数组数目"><a href="#大小为-K-且平均值大于等于阈值的子数组数目" class="headerlink" title="大小为 K 且平均值大于等于阈值的子数组数目"></a>大小为 K 且平均值大于等于阈值的子数组数目</h4><h5 id="题目"><a href="#题目" class="headerlink" title="题目"></a><a href="https://leetcode-cn.com/problems/number-of-sub-arrays-of-size-k-and-average-greater-than-or-equal-to-threshold/" target="_blank" rel="noopener">题目</a></h5><p>给你一个整数数组 <code>arr</code> 和两个整数 <code>k</code> 和 <code>threshold</code> 。</p>
<p>请你返回长度为 <code>k</code> 且平均值大于等于 <code>threshold</code> 的子数组数目。</p>
<p><strong>示例</strong></p>
<figure class="highlight tex"><table><tr><td class="code"><pre><span class="line">输入：arr = [2,2,2,2,5,5,5,8], k = 3, threshold = 4</span><br><span class="line">输出：3</span><br><span class="line">解释：子数组 [2,5,5],[5,5,5] 和 [5,5,8] 的平均值分别为 4，5 和 6 。其他长度为 3 的子数组的平均值都小于 4 （threshold 的值)。</span><br><span class="line"></span><br><span class="line">输入：arr = [1,1,1,1,1], k = 1, threshold = 0</span><br><span class="line">输出：5</span><br><span class="line"></span><br><span class="line">输入：arr = [11,13,17,23,29,31,7,5,2,3], k = 3, threshold = 5</span><br><span class="line">输出：6</span><br><span class="line">解释：前 6 个长度为 3 的子数组平均值都大于 5 。注意平均值不是整数。</span><br><span class="line"></span><br><span class="line">输入：arr = [7,7,7,7,7,7,7], k = 7, threshold = 7</span><br><span class="line">输出：1</span><br><span class="line"></span><br><span class="line">输入：arr = [4,4,4,4], k = 4, threshold = 1</span><br><span class="line">输出：1</span><br></pre></td></tr></table></figure>

<h5 id="题解"><a href="#题解" class="headerlink" title="题解"></a>题解</h5><p>思路：做子数组时优先考虑前缀和和哈希表，前缀和能减少时间复杂夫</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">numOfSubarrays</span><span class="params">(self, arr: List[int], k: int, threshold: int)</span> -&gt; int:</span></span><br><span class="line">        count = <span class="number">0</span></span><br><span class="line">        p = [<span class="number">0</span> <span class="keyword">for</span> x <span class="keyword">in</span> range(len(arr)+<span class="number">1</span>)]</span><br><span class="line">        <span class="keyword">for</span> i, num <span class="keyword">in</span> enumerate(arr):</span><br><span class="line">            p[i+<span class="number">1</span>] = p[i] + num</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(k, len(p)):</span><br><span class="line">            dec = p[i] - p[i-k]</span><br><span class="line">            avg = dec / k</span><br><span class="line">            <span class="keyword">if</span> avg &gt;= threshold:</span><br><span class="line">                count += <span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> count</span><br></pre></td></tr></table></figure>

<p>时间复杂度：O(N)</p>
<p>空间复杂度：O(N)</p>
<hr>
<p><u>未完待续。。。</u></p>
]]></content>
      <categories>
        <category>LeetCode</category>
        <category>前缀和</category>
      </categories>
      <tags>
        <tag>LeetCode</tag>
      </tags>
  </entry>
  <entry>
    <title>leetcode栈</title>
    <url>/2020/05/31/leetcode%E6%A0%88/</url>
    <content><![CDATA[<p>栈</p>
<ul>
<li><p><u>柱状图最大的矩形</u></p>
</li>
<li><p>字符串解码</p>
</li>
</ul>
<a id="more"></a>

<h4 id="柱状图最大的矩形"><a href="#柱状图最大的矩形" class="headerlink" title="柱状图最大的矩形"></a>柱状图最大的矩形</h4><h5 id="题目"><a href="#题目" class="headerlink" title="题目"></a><a href="https://leetcode-cn.com/problems/largest-rectangle-in-histogram/" target="_blank" rel="noopener">题目</a></h5><p>给定 n 个非负整数，用来表示柱状图中各个柱子的高度。每个柱子彼此相邻，且宽度为 1 。</p>
<p>求在该柱状图中，能够勾勒出来的矩形的最大面积。</p>
<p><img src="https://assets.leetcode-cn.com/aliyun-lc-upload/uploads/2018/10/12/histogram.png" alt="img"></p>
<p>以上是柱状图的示例，其中每个柱子的宽度为 1，给定的高度为 <code>[2,1,5,6,2,3]</code>。</p>
<p><img src="https://assets.leetcode-cn.com/aliyun-lc-upload/uploads/2018/10/12/histogram_area.png" alt="img"></p>
<p>阴影部分为所能勾勒出的最大矩形面积，其面积为 <code>10</code> 个单位。</p>
<p><strong>示例</strong></p>
<figure class="highlight tex"><table><tr><td class="code"><pre><span class="line">输入: [2,1,5,6,2,3]</span><br><span class="line">输出: 10</span><br></pre></td></tr></table></figure>

<h5 id="题解"><a href="#题解" class="headerlink" title="题解"></a>题解</h5><h6 id="方法一（动态规划）"><a href="#方法一（动态规划）" class="headerlink" title="方法一（动态规划）"></a>方法一（动态规划）</h6><p>思路：</p>
<ol>
<li>初始化max_A为0</li>
<li>遍历数组，遍历到每一个数组时，去判断当前位置能围成的最大的矩形</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">largestRectangleArea</span><span class="params">(self, heights: List[int])</span> -&gt; int:</span></span><br><span class="line">        max_a = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(len(heights)):</span><br><span class="line">            max_a = max(max_a, heights[i])</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> range(i):</span><br><span class="line">                max_a = max((i-j+<span class="number">1</span>) * min(heights[j:i+<span class="number">1</span>]), max_a)</span><br><span class="line">        <span class="keyword">return</span> max_a</span><br></pre></td></tr></table></figure>

<p>时间复杂度：O(n^2)</p>
<p>空间复杂度：O(1)</p>
<h6 id="方法二（单调栈）"><a href="#方法二（单调栈）" class="headerlink" title="方法二（单调栈）"></a>方法二（单调栈）</h6><p>思路：遍历高度数组。如果当前h[i]比栈顶元素要大，则将当前遍历到的h[i]入栈；否则将栈顶元素出栈，并计算其高度下围成的矩形面积。重复这一过程，直到栈空</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">largestRectangleArea</span><span class="params">(self, heights: List[int])</span> -&gt; int:</span></span><br><span class="line">        n = len(heights)</span><br><span class="line">        stack = []</span><br><span class="line">        left = [<span class="number">0</span> <span class="keyword">for</span> x <span class="keyword">in</span> heights]</span><br><span class="line">        right = [n <span class="keyword">for</span> x <span class="keyword">in</span> heights]</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(n):</span><br><span class="line">            <span class="keyword">while</span> stack <span class="keyword">and</span> heights[stack[<span class="number">-1</span>]] &gt;= heights[i]:</span><br><span class="line">                right[stack[<span class="number">-1</span>]] = i</span><br><span class="line">                stack.pop()</span><br><span class="line">            left[i] = stack[<span class="number">-1</span>] <span class="keyword">if</span> stack <span class="keyword">else</span> <span class="number">-1</span></span><br><span class="line">            stack.append(i)</span><br><span class="line">        ans = max((right[i] - left[i] - <span class="number">1</span>) * heights[i] <span class="keyword">for</span> i <span class="keyword">in</span> range(n)) <span class="keyword">if</span> n &gt; <span class="number">0</span> <span class="keyword">else</span> <span class="number">0</span></span><br><span class="line">        <span class="keyword">return</span> ans</span><br></pre></td></tr></table></figure>

<p>时间复杂度：O(N)</p>
<p>空间复杂度：O(N)</p>
<hr>
<h4 id="字符串解码"><a href="#字符串解码" class="headerlink" title="字符串解码"></a>字符串解码</h4><h5 id="题目-1"><a href="#题目-1" class="headerlink" title="题目"></a><a href="https://leetcode-cn.com/problems/decode-string/" target="_blank" rel="noopener">题目</a></h5><p>给定一个经过编码的字符串，返回它解码后的字符串。</p>
<p>编码规则为: k[encoded_string]，表示其中方括号内部的 encoded_string 正好重复 k 次。注意 k 保证为正整数。</p>
<p>你可以认为输入字符串总是有效的；输入字符串中没有额外的空格，且输入的方括号总是符合格式要求的。</p>
<p>此外，你可以认为原始数据不包含数字，所有的数字只表示重复的次数 k ，例如不会出现像 <code>3a</code> 或 <code>2[4]</code> 的输入。</p>
<p><strong>示例</strong></p>
<figure class="highlight tex"><table><tr><td class="code"><pre><span class="line">s = "3[a]2[bc]", 返回 "aaabcbc".</span><br><span class="line">s = "3[a2[c]]", 返回 "accaccacc".</span><br><span class="line">s = "2[abc]3[cd]ef", 返回 "abcabccdcdcdef".</span><br></pre></td></tr></table></figure>

<h5 id="题解-1"><a href="#题解-1" class="headerlink" title="题解"></a>题解</h5><h6 id="方法一（辅助栈）"><a href="#方法一（辅助栈）" class="headerlink" title="方法一（辅助栈）"></a>方法一（辅助栈）</h6><p>思路：</p>
<ol>
<li>定义一个辅助栈用来存放<code>[</code>前读到的数字（重复的倍数）和在这之前解码的字符串</li>
<li>遍历字符串，当一直是数字时，记录数字的值；当遇到<code>[</code>时，则将已读取的数字和当前解码出的字符串入栈，当读取的时字符时，记录下字符的值；当读取<code>]</code>时，将栈顶元素出栈，此时将保存的倍数及当前读取的字符串相乘加入已解码的字符串。</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">decodeString</span><span class="params">(self, s: str)</span> -&gt; str:</span></span><br><span class="line">        <span class="comment"># 辅助栈</span></span><br><span class="line">        stack, res = [], <span class="string">""</span></span><br><span class="line">        multi = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> c <span class="keyword">in</span> s:</span><br><span class="line">            <span class="keyword">if</span> c == <span class="string">'['</span>:</span><br><span class="line">                stack.append([multi, res])</span><br><span class="line">                res, multi = <span class="string">""</span>, <span class="number">0</span></span><br><span class="line">            <span class="keyword">elif</span> c == <span class="string">']'</span>:</span><br><span class="line">                last_multi, last_res = stack.pop()</span><br><span class="line">                res = last_res + last_multi * res</span><br><span class="line">            <span class="keyword">elif</span> <span class="string">'0'</span> &lt;= c &lt;= <span class="string">'9'</span>:</span><br><span class="line">                multi = <span class="number">10</span> * multi + int(c)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                res += c</span><br><span class="line">        <span class="keyword">return</span> res</span><br></pre></td></tr></table></figure>

<p>时间复杂度: O(N)</p>
<p>空间复杂度: O(N)</p>
<h6 id="方法二（递归）"><a href="#方法二（递归）" class="headerlink" title="方法二（递归）"></a><u>方法二（递归）</u></h6><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">decodeString</span><span class="params">(self, s: str)</span> -&gt; str:</span></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">dfs</span><span class="params">(s, i)</span>:</span></span><br><span class="line">            res, multi = <span class="string">""</span>, <span class="number">0</span></span><br><span class="line">            <span class="keyword">while</span> i &lt; len(s):</span><br><span class="line">                <span class="keyword">if</span> <span class="string">'0'</span> &lt;= s[i] &lt;= <span class="string">'9'</span>:</span><br><span class="line">                    multi = <span class="number">10</span> * multi + int(s[i])</span><br><span class="line">                <span class="keyword">elif</span> s[i] == <span class="string">'['</span>:</span><br><span class="line">                    i, tmp = dfs(s, i+<span class="number">1</span>)</span><br><span class="line">                    res = res + multi * tmp</span><br><span class="line">                    multi = <span class="number">0</span></span><br><span class="line">                <span class="keyword">elif</span> s[i] == <span class="string">']'</span>:</span><br><span class="line">                    <span class="keyword">return</span> i, res</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    res += s[i]</span><br><span class="line">                i+=<span class="number">1</span></span><br><span class="line">            <span class="keyword">return</span> res</span><br><span class="line">        <span class="keyword">return</span> dfs(s, <span class="number">0</span>)</span><br></pre></td></tr></table></figure>

<p>时间复杂度: O(N)</p>
<p>空间复杂度: O(N)</p>
<hr>
<p>未完待续。。。</p>
]]></content>
      <categories>
        <category>LeetCode</category>
        <category>栈</category>
      </categories>
      <tags>
        <tag>LeetCode</tag>
      </tags>
  </entry>
  <entry>
    <title>leetcode二叉树</title>
    <url>/2020/05/31/leetcode%E4%BA%8C%E5%8F%89%E6%A0%91/</url>
    <content><![CDATA[<p>二叉树</p>
<ul>
<li><u>二叉树的前序遍历</u></li>
<li><u>重建二叉树</u></li>
</ul>
<a id="more"></a>

<h4 id="二叉树的前序遍历"><a href="#二叉树的前序遍历" class="headerlink" title="二叉树的前序遍历"></a>二叉树的前序遍历</h4><h5 id="题目"><a href="#题目" class="headerlink" title="题目"></a><a href="https://leetcode-cn.com/problems/binary-tree-preorder-traversal/" target="_blank" rel="noopener">题目</a></h5><p>给定一个二叉树，返回它的 <em>前序</em> 遍历。</p>
<p><strong>示例</strong></p>
<figure class="highlight tex"><table><tr><td class="code"><pre><span class="line">输入: [1,null,2,3]  </span><br><span class="line">   1</span><br><span class="line">    <span class="tag">\</span></span><br><span class="line">     2</span><br><span class="line">    /</span><br><span class="line">   3 </span><br><span class="line"></span><br><span class="line">输出: [1,2,3]</span><br></pre></td></tr></table></figure>

<h5 id="解题"><a href="#解题" class="headerlink" title="解题"></a>解题</h5><h6 id="方法一"><a href="#方法一" class="headerlink" title="方法一"></a>方法一</h6><p>思路：用递归的思想</p>
<ol>
<li>访问根节点</li>
<li>访问左子树</li>
<li>访问右子树</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">preorderTraversal</span><span class="params">(self, root: TreeNode)</span> -&gt; List[int]:</span></span><br><span class="line">        res = []</span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">pT</span><span class="params">(root)</span>:</span></span><br><span class="line">            <span class="keyword">nonlocal</span> res</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> root:</span><br><span class="line">                <span class="keyword">return</span></span><br><span class="line">            res.append(root.val)</span><br><span class="line">            pT(root.left)</span><br><span class="line">            pT(root.right)</span><br><span class="line">        pT(root)</span><br><span class="line">        <span class="keyword">return</span> res</span><br></pre></td></tr></table></figure>

<p>其他方法会持续更新。。。</p>
<hr>
<h4 id="重建二叉树"><a href="#重建二叉树" class="headerlink" title="重建二叉树"></a>重建二叉树</h4><h5 id="题目-1"><a href="#题目-1" class="headerlink" title="题目"></a><a href="https://leetcode-cn.com/problems/zhong-jian-er-cha-shu-lcof/" target="_blank" rel="noopener">题目</a></h5><p>输入某二叉树的前序遍历和中序遍历的结果，请重建该二叉树。假设输入的前序遍历和中序遍历的结果中都不含重复的数字。</p>
<p><strong>示例</strong></p>
<figure class="highlight tex"><table><tr><td class="code"><pre><span class="line">前序遍历 preorder = [3,9,20,15,7]</span><br><span class="line">中序遍历 inorder = [9,3,15,20,7]</span><br><span class="line"></span><br><span class="line">    3</span><br><span class="line">   / <span class="tag">\</span></span><br><span class="line">  9  20</span><br><span class="line">    /  <span class="tag">\</span></span><br><span class="line">   15   7</span><br></pre></td></tr></table></figure>

<h5 id="题解"><a href="#题解" class="headerlink" title="题解"></a>题解</h5><h6 id="方法一（递归）"><a href="#方法一（递归）" class="headerlink" title="方法一（递归）"></a>方法一（递归）</h6><p>思路：</p>
<ol>
<li>根据前序遍历的顺序找出当前树的根节点</li>
<li>在中序遍历的数组中找出根节点，左边的即为左子树，右边的即为右子树</li>
<li>递归上述过程</li>
<li>递归的结束条件是当前序（中序）数组长度为0的时候，返回空</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">buildTree</span><span class="params">(self, preorder: List[int], inorder: List[int])</span> -&gt; TreeNode:</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> len(preorder):</span><br><span class="line">            <span class="keyword">return</span></span><br><span class="line">        Tree = TreeNode(preorder[<span class="number">0</span>])</span><br><span class="line">        key = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> index, i <span class="keyword">in</span> enumerate(inorder):</span><br><span class="line">            <span class="keyword">if</span> i == preorder[<span class="number">0</span>]:</span><br><span class="line">                key = index</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">        pre_left = preorder[<span class="number">1</span>:key+<span class="number">1</span>]</span><br><span class="line">        pre_right = preorder[key+<span class="number">1</span>:]</span><br><span class="line">        in_left = inorder[:key]</span><br><span class="line">        in_right = inorder[key+<span class="number">1</span>:]</span><br><span class="line">        Tree.left = self.buildTree(pre_left, in_left)</span><br><span class="line">        Tree.right = self.buildTree(pre_right, in_right)</span><br><span class="line">        <span class="keyword">return</span> Tree</span><br></pre></td></tr></table></figure>

<p>时间复杂度O(N)</p>
<p>空间复杂度O(N)</p>
<h6 id="方法二（迭代）"><a href="#方法二（迭代）" class="headerlink" title="方法二（迭代）"></a>方法二（迭代）</h6><p>持续更新中。。。</p>
<hr>
<p><u>未完待续</u></p>
]]></content>
      <categories>
        <category>LeetCode</category>
        <category>二叉树</category>
      </categories>
      <tags>
        <tag>LeetCode</tag>
      </tags>
  </entry>
  <entry>
    <title>leetcode哈希表</title>
    <url>/2020/05/31/leetcode%E5%93%88%E5%B8%8C%E8%A1%A8/</url>
    <content><![CDATA[<p>哈希表</p>
<ul>
<li>数对和</li>
<li>出现次数最多的子树元素和</li>
<li>和可被 K 整除的子数组</li>
<li><u>函数的独占时间</u></li>
</ul>
<a id="more"></a>

<h4 id="数对和"><a href="#数对和" class="headerlink" title="数对和"></a>数对和</h4><h5 id="题目"><a href="#题目" class="headerlink" title="题目"></a><a href="https://leetcode-cn.com/problems/pairs-with-sum-lcci/" target="_blank" rel="noopener">题目</a></h5><p><strong>示例</strong></p>
<figure class="highlight tex"><table><tr><td class="code"><pre><span class="line">输入: nums = [5,6,5], target = 11</span><br><span class="line">输出: [[5,6]]</span><br><span class="line"></span><br><span class="line">输入: nums = [5,6,5,6], target = 11</span><br><span class="line">输出: [[5,6],[5,6]]</span><br></pre></td></tr></table></figure>

<h5 id="题解"><a href="#题解" class="headerlink" title="题解"></a>题解</h5><p>思路：</p>
<ol>
<li>用哈希表记录每个元素的出现次数</li>
<li>遍历数组，当前哈希表中该键值于(target-该值)的value减一，然后判断两个键值是否均<strong>大于等于</strong>0，如果是，则说明还存在数对，记录到最终结果中</li>
</ol>
<p>注意：大于等于；先减一，后判断</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">pairSums</span><span class="params">(self, nums: List[int], target: int)</span> -&gt; List[List[int]]:</span></span><br><span class="line">        res = []</span><br><span class="line">        record = &#123;&#125;</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> nums:</span><br><span class="line">            record[i] = record.get(i, <span class="number">0</span>) + <span class="number">1</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> nums:</span><br><span class="line">            record[i] = record.get(i, <span class="number">0</span>) - <span class="number">1</span></span><br><span class="line">            record[target-i] = record.get(target-i, <span class="number">0</span>) - <span class="number">1</span></span><br><span class="line">            <span class="keyword">if</span> record.get(target-i, <span class="number">0</span>) &gt;= <span class="number">0</span> <span class="keyword">and</span> record.get(i, <span class="number">0</span>) &gt;= <span class="number">0</span>:</span><br><span class="line">                res.append([i, target-i])</span><br><span class="line">        <span class="keyword">return</span> res</span><br></pre></td></tr></table></figure>

<hr>
<h4 id="出现次数最多的子树元素和"><a href="#出现次数最多的子树元素和" class="headerlink" title="出现次数最多的子树元素和"></a>出现次数最多的子树元素和</h4><h5 id="题目-1"><a href="#题目-1" class="headerlink" title="题目"></a><a href="https://leetcode-cn.com/problems/most-frequent-subtree-sum/" target="_blank" rel="noopener">题目</a></h5><p>给你一个二叉树的根结点，请你找出出现次数最多的子树元素和。一个结点的「子树元素和」定义为以该结点为根的二叉树上所有结点的元素之和（包括结点本身）。</p>
<p>你需要返回出现次数最多的子树元素和。如果有多个元素出现的次数相同，返回所有出现次数最多的子树元素和（不限顺序）。</p>
<p><strong>示例</strong></p>
<figure class="highlight tex"><table><tr><td class="code"><pre><span class="line">  5</span><br><span class="line"> /  <span class="tag">\</span></span><br><span class="line">2   -3</span><br><span class="line">返回 [2, -3, 4]，所有的值均只出现一次，以任意顺序返回所有值。</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  5</span><br><span class="line"> /  <span class="tag">\</span></span><br><span class="line">2   -5</span><br><span class="line">返回 [2]，只有 2 出现两次，-5 只出现 1 次。</span><br></pre></td></tr></table></figure>

<h5 id="题解-1"><a href="#题解-1" class="headerlink" title="题解"></a>题解</h5><p>思路：</p>
<ol>
<li>首先掌握树的遍历，（深度优先[前序，中序，后序]，广度优先），在树专题会有介绍</li>
<li>遍历过程中记录当前数的和，可表示为（树的根节点值+左子树和+右子树和），用哈希表存储</li>
<li>对比哈希表中的value值，将所有最大值的保存至结果</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">findFrequentTreeSum</span><span class="params">(self, root: TreeNode)</span> -&gt; List[int]:</span></span><br><span class="line">        res = &#123;&#125;</span><br><span class="line">        ret = []</span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">dfs</span><span class="params">(root)</span>:</span></span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> root:</span><br><span class="line">                <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">            sum_now = dfs(root.left) + dfs(root.right) + root.val</span><br><span class="line">            res[sum_now] = res.get(sum_now, <span class="number">0</span>) + <span class="number">1</span></span><br><span class="line">            <span class="keyword">return</span> sum_now</span><br><span class="line">        dfs(root)</span><br><span class="line">        <span class="keyword">for</span> (key, value) <span class="keyword">in</span> res.items():</span><br><span class="line">            <span class="keyword">if</span> value == max(res.values()):</span><br><span class="line">                ret.append(key)</span><br><span class="line">        <span class="keyword">return</span> ret</span><br></pre></td></tr></table></figure>

<p>时间复杂度是使用遍历树方法的时间复杂度</p>
<p>空间复杂度是O(N)</p>
<hr>
<h4 id="和可被-K-整除的子数组"><a href="#和可被-K-整除的子数组" class="headerlink" title="和可被 K 整除的子数组"></a>和可被 K 整除的子数组</h4><h5 id="题目-2"><a href="#题目-2" class="headerlink" title="题目"></a><a href="https://leetcode-cn.com/problems/subarray-sum-equals-k/" target="_blank" rel="noopener">题目</a></h5><p>给定一个整数数组和一个整数 <strong>k，</strong>你需要找到该数组中和为 <strong>k</strong> 的连续的子数组的个数。</p>
<p><strong>示例</strong></p>
<figure class="highlight tex"><table><tr><td class="code"><pre><span class="line">输入:nums = [1,1,1], k = 2</span><br><span class="line">输出: 2 , [1,1] 与 [1,1] 为两种不同的情况。</span><br></pre></td></tr></table></figure>

<h5 id="题解-2"><a href="#题解-2" class="headerlink" title="题解"></a>题解</h5><p>思路：</p>
<ol>
<li>利用前缀和可以优化时间复杂度</li>
<li>利用哈希表可以很轻松的记录sum-k的键值出现多少次</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">subarraySum</span><span class="params">(self, nums: List[int], k: int)</span> -&gt; int:</span></span><br><span class="line">        hashmap = &#123;<span class="number">0</span>:<span class="number">1</span>&#125;</span><br><span class="line">        count = <span class="number">0</span></span><br><span class="line">        sums = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(len(nums)):</span><br><span class="line">            sums += nums[i]</span><br><span class="line">            <span class="keyword">if</span> sums-k <span class="keyword">in</span> hashmap:</span><br><span class="line">                count = count + hashmap[sums-k]</span><br><span class="line">            <span class="keyword">if</span> sums <span class="keyword">in</span> hashmap:</span><br><span class="line">                hashmap[sums] += <span class="number">1</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                hashmap[sums] = <span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> count</span><br></pre></td></tr></table></figure>

<p><em>注意仔细理解<code>hashmap = {0:1}</code>（考虑了前缀和刚好是k），必要的话可以举例。</em></p>
<p>时间复杂度是O(N)</p>
<p>空间复杂度是O(N)</p>
<hr>
<h4 id="函数的独占时间"><a href="#函数的独占时间" class="headerlink" title="函数的独占时间"></a>函数的独占时间</h4><h5 id="题目-3"><a href="#题目-3" class="headerlink" title="题目"></a><a href="https://leetcode-cn.com/problems/exclusive-time-of-functions/" target="_blank" rel="noopener">题目</a></h5><p>给出一个非抢占单线程CPU的 n 个函数运行日志，找到函数的独占时间。</p>
<p>每个函数都有一个唯一的 Id，从 0 到 n-1，函数可能会递归调用或者被其他函数调用。</p>
<p>日志是具有以下格式的字符串：<code>function_id：start_or_end：timestamp</code>。例如：”<code>0:start:0</code>“ 表示函数 0 从 0 时刻开始运行。<code>&quot;0:end:0&quot;</code> 表示函数 0 在 0 时刻结束。</p>
<p>函数的独占时间定义是在该方法中花费的时间，调用其他函数花费的时间不算该函数的独占时间。你需要根据函数的 Id 有序地返回每个函数的独占时间。</p>
<p><strong>示例</strong></p>
<figure class="highlight tex"><table><tr><td class="code"><pre><span class="line">输入:</span><br><span class="line">n = 2</span><br><span class="line">logs = </span><br><span class="line">["0:start:0",</span><br><span class="line"> "1:start:2",</span><br><span class="line"> "1:end:5",</span><br><span class="line"> "0:end:6"]</span><br><span class="line">输出:[3, 4]</span><br><span class="line">说明：</span><br><span class="line">函数 0 在时刻 0 开始，在执行了  2个时间单位结束于时刻 1。</span><br><span class="line">现在函数 0 调用函数 1，函数 1 在时刻 2 开始，执行 4 个时间单位后结束于时刻 5。</span><br><span class="line">函数 0 再次在时刻 6 开始执行，并在时刻 6 结束运行，从而执行了 1 个时间单位。</span><br><span class="line">所以函数 0 总共的执行了 2 +1 =3 个时间单位，函数 1 总共执行了 4 个时间单位。</span><br></pre></td></tr></table></figure>

<p>说明：</p>
<ol>
<li>输入的日志会根据时间戳排序，而不是根据日志Id排序。</li>
<li>你的输出会根据函数Id排序，也就意味着你的输出数组中序号为 0 的元素相当于函数 0 的执行时间。</li>
<li>两个函数不会在同时开始或结束。</li>
<li>函数允许被递归调用，直到运行结束。</li>
<li>1 &lt;= n &lt;= 100</li>
</ol>
<h5 id="解题"><a href="#解题" class="headerlink" title="解题"></a>解题</h5><p>思路：</p>
<ol>
<li>用栈模拟函数的调用，遇到start的日志时，对应的id入栈；遇到end的日志时，对应的id出栈。栈顶函数是正在执行的函数。</li>
<li>用哈希表记录每个函数的独占时间，key代表id，value代表时间</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">exclusiveTime</span><span class="params">(self, n: int, logs: List[str])</span> -&gt; List[int]:</span></span><br><span class="line">        L = [log.split(<span class="string">':'</span>) <span class="keyword">for</span> log <span class="keyword">in</span> logs]</span><br><span class="line">        stack, res = [], [<span class="number">0</span>] * n</span><br><span class="line">        prev = <span class="number">-1</span></span><br><span class="line">        start=<span class="literal">True</span></span><br><span class="line">        <span class="keyword">for</span> (T_id, status, time) <span class="keyword">in</span>  L:</span><br><span class="line">            <span class="keyword">if</span> status == <span class="string">'start'</span>:</span><br><span class="line">                <span class="keyword">if</span> stack:</span><br><span class="line">                    last_id, last_time = stack[<span class="number">-1</span>]</span><br><span class="line">                    last_id = int(last_id)</span><br><span class="line">                    last_time = int(last_time)</span><br><span class="line">                    <span class="keyword">if</span> start:</span><br><span class="line">                        res[last_id] = res[last_id] + int(time) - last_time</span><br><span class="line">                    <span class="keyword">else</span>:</span><br><span class="line">                        res[last_id] = res[last_id] + int(time) - prev  - <span class="number">1</span></span><br><span class="line">                    start = <span class="literal">True</span></span><br><span class="line">                stack.append([T_id, time])</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                last_id, last_time = stack[<span class="number">-1</span>]</span><br><span class="line">                last_id = int(last_id)</span><br><span class="line">                last_time = int(last_time)</span><br><span class="line">                <span class="keyword">if</span> start:</span><br><span class="line">                    res[last_id] += int(time) - last_time + <span class="number">1</span>      </span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    res[last_id] += int(time) - prev</span><br><span class="line">                start = <span class="literal">False</span></span><br><span class="line">                stack.pop()</span><br><span class="line">                prev = int(time)</span><br><span class="line">        <span class="keyword">return</span> [res[i] <span class="keyword">for</span> i <span class="keyword">in</span> range(n)]</span><br></pre></td></tr></table></figure>

<p>时间复杂度O(N)</p>
<p>空间复杂度O(N)</p>
<hr>
<p><u>未完待续</u></p>
]]></content>
      <categories>
        <category>LeetCode</category>
        <category>哈希表</category>
      </categories>
      <tags>
        <tag>LeetCode</tag>
      </tags>
  </entry>
  <entry>
    <title>leetcode分情况讨论</title>
    <url>/2020/05/31/leetcode%E5%88%86%E6%83%85%E5%86%B5%E8%AE%A8%E8%AE%BA/</url>
    <content><![CDATA[<p>分情况讨论</p>
<ul>
<li>井字游戏</li>
<li>递减元素使数组呈锯齿状</li>
</ul>
<a id="more"></a>

<h4 id="井字游戏"><a href="#井字游戏" class="headerlink" title="井字游戏"></a>井字游戏</h4><h5 id="题目"><a href="#题目" class="headerlink" title="题目"></a><a href="https://leetcode-cn.com/problems/tic-tac-toe-lcci/" target="_blank" rel="noopener">题目</a></h5><p>设计一个算法，判断玩家是否赢了井字游戏。输入是一个 N x N 的数组棋盘，由字符” “，”X”和”O”组成，其中字符” “代表一个空位。</p>
<p>以下是井字游戏的规则：</p>
<ul>
<li>玩家轮流将字符放入空位（” “）中。</li>
<li>第一个玩家总是放字符”O”，且第二个玩家总是放字符”X”。</li>
<li>“X”和”O”只允许放置在空位中，不允许对已放有字符的位置进行填充。</li>
<li>当有N个相同（且非空）的字符填充任何行、列或对角线时，游戏结束，对应该字符的玩家获胜。</li>
<li>当所有位置非空时，也算为游戏结束。</li>
<li>如果游戏结束，玩家不允许再放置字符。</li>
</ul>
<p>如果游戏存在获胜者，就返回该游戏的获胜者使用的字符（”X”或”O”）；如果游戏以平局结束，则返回 “Draw”；如果仍会有行动（游戏未结束），则返回 “Pending”。</p>
<p><strong>示例</strong></p>
<figure class="highlight tex"><table><tr><td class="code"><pre><span class="line">输入： board = ["O X"," XO","X O"]</span><br><span class="line">输出： "X"</span><br><span class="line"></span><br><span class="line">输入： board = ["OOX","XXO","OXO"]</span><br><span class="line">输出： "Draw"</span><br><span class="line">解释： 没有玩家获胜且不存在空位</span><br><span class="line"></span><br><span class="line">输入： board = ["OOX","XXO","OX "]</span><br><span class="line">输出： "Pending"</span><br><span class="line">解释： 没有玩家获胜且仍存在空位</span><br></pre></td></tr></table></figure>

<h5 id="题解"><a href="#题解" class="headerlink" title="题解"></a>题解</h5><p>思路：</p>
<p>本题一共有四种情况</p>
<ul>
<li>“X”获胜</li>
<li>“O”获胜</li>
<li>没有玩家获胜且没有空位</li>
<li>没有玩家获胜有空位</li>
</ul>
<p><strong>我们将X替换为-1，O替换为1，对数组的行，列，对角线求和。如果出现-3或3则判定X或O获胜；若没有出现，则进一步判定空位是否存在</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">tictactoe</span><span class="params">(self, board: List[str])</span> -&gt; str:</span></span><br><span class="line">    n = len(board)</span><br><span class="line">    arr = [[<span class="number">0</span> <span class="keyword">for</span> x <span class="keyword">in</span> board] <span class="keyword">for</span> y <span class="keyword">in</span> range(len(board))] </span><br><span class="line">    raw = [<span class="number">0</span> <span class="keyword">for</span> x <span class="keyword">in</span> board]</span><br><span class="line">    col = [<span class="number">0</span> <span class="keyword">for</span> x <span class="keyword">in</span> board]</span><br><span class="line">    left_right = <span class="number">0</span></span><br><span class="line">    right_left = <span class="number">0</span></span><br><span class="line">    ending = <span class="literal">True</span></span><br><span class="line">    <span class="keyword">for</span> i,line <span class="keyword">in</span> enumerate(board):</span><br><span class="line">        <span class="keyword">for</span> j, ch <span class="keyword">in</span> enumerate(line):</span><br><span class="line">            <span class="keyword">if</span> ch == <span class="string">'O'</span>:</span><br><span class="line">                arr[i][j] = <span class="number">1</span></span><br><span class="line">            <span class="keyword">elif</span> ch == <span class="string">'X'</span>:</span><br><span class="line">                arr[i][j] = <span class="number">-1</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                ending = <span class="literal">False</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(arr)):</span><br><span class="line">        raw[i] = sum(arr[i])</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(arr)):</span><br><span class="line">        left_right += arr[i][i]</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(arr)):</span><br><span class="line">        right_left += arr[i][len(arr)-i<span class="number">-1</span>]</span><br><span class="line">    arr = [[row[i] <span class="keyword">for</span> row <span class="keyword">in</span> arr] <span class="keyword">for</span> i <span class="keyword">in</span> range(len(arr[<span class="number">0</span>]))]</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(arr)):</span><br><span class="line">        col[i] = sum(arr[i])</span><br><span class="line">    X = raw + col</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> (n <span class="keyword">in</span> raw) <span class="keyword">or</span> (n <span class="keyword">in</span> col) <span class="keyword">or</span> (left_right == n) <span class="keyword">or</span> (right_left == n):</span><br><span class="line">        <span class="keyword">return</span> <span class="string">'O'</span></span><br><span class="line">    <span class="keyword">if</span> (-n <span class="keyword">in</span> raw) <span class="keyword">or</span> (-n <span class="keyword">in</span> col) <span class="keyword">or</span> (left_right == -n) <span class="keyword">or</span> (right_left == -n):</span><br><span class="line">        <span class="keyword">return</span> <span class="string">'X'</span></span><br><span class="line">    <span class="keyword">if</span> ending == <span class="literal">False</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="string">'Pending'</span></span><br><span class="line">    <span class="keyword">if</span> ending == <span class="literal">True</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="string">'Draw'</span></span><br></pre></td></tr></table></figure>

<p>时间复杂度O(n^2),空间复杂度O(n)</p>
<hr>
<h4 id="递减元素使数组呈锯齿状"><a href="#递减元素使数组呈锯齿状" class="headerlink" title="递减元素使数组呈锯齿状"></a>递减元素使数组呈锯齿状</h4><h5 id="题目-1"><a href="#题目-1" class="headerlink" title="题目"></a><a href="https://leetcode-cn.com/problems/decrease-elements-to-make-array-zigzag/" target="_blank" rel="noopener">题目</a></h5><p>给你一个整数数组 nums，每次 操作 会从中选择一个元素并 将该元素的值减少 1。</p>
<p>如果符合下列情况之一，则数组 A 就是 锯齿数组：</p>
<ul>
<li>每个偶数索引对应的元素都大于相邻的元素，即 A[0] &gt; A[1] &lt; A[2] &gt; A[3] &lt; A[4] &gt; …</li>
<li>或者，每个奇数索引对应的元素都大于相邻的元素，即 A[0] &lt; A[1] &gt; A[2] &lt; A[3] &gt; A[4] &lt; …</li>
</ul>
<p>返回将数组 nums 转换为锯齿数组所需的最小操作次数。</p>
<p><strong>示例</strong></p>
<figure class="highlight tex"><table><tr><td class="code"><pre><span class="line">输入：nums = [1,2,3]</span><br><span class="line">输出：2</span><br><span class="line">解释：我们可以把 2 递减到 0，或把 3 递减到 1。</span><br><span class="line"></span><br><span class="line">输入：nums = [9,6,1,6,2]</span><br><span class="line">输出：4</span><br></pre></td></tr></table></figure>

<h5 id="题解-1"><a href="#题解-1" class="headerlink" title="题解"></a>题解</h5><p>思路：分情况讨论奇数位置和偶数位置</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">movesToMakeZigzag</span><span class="params">(self, nums: List[int])</span> -&gt; int:</span></span><br><span class="line">        n = len(nums)</span><br><span class="line">        ans1, ans2 = <span class="number">0</span>, <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(n):</span><br><span class="line">            <span class="comment"># 奇数位置</span></span><br><span class="line">            <span class="keyword">if</span> i % <span class="number">2</span> == <span class="number">0</span>:</span><br><span class="line">                d1 = nums[i] - nums[i - <span class="number">1</span>] + <span class="number">1</span> <span class="keyword">if</span> i &gt; <span class="number">0</span> <span class="keyword">and</span> nums[i] &gt;= nums[i - <span class="number">1</span>] <span class="keyword">else</span> <span class="number">0</span></span><br><span class="line">                d2 = nums[i] - nums[i + <span class="number">1</span>] + <span class="number">1</span> <span class="keyword">if</span> i &lt; n - <span class="number">1</span> <span class="keyword">and</span> nums[i] &gt;= nums[i + <span class="number">1</span>] <span class="keyword">else</span> <span class="number">0</span></span><br><span class="line">                ans1 += max(d1, d2)</span><br><span class="line">            <span class="comment"># 偶数位置</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                d1 = nums[i] - nums[i - <span class="number">1</span>] + <span class="number">1</span> <span class="keyword">if</span> nums[i] &gt;= nums[i - <span class="number">1</span>] <span class="keyword">else</span> <span class="number">0</span></span><br><span class="line">                d2 = nums[i] - nums[i + <span class="number">1</span>] + <span class="number">1</span> <span class="keyword">if</span> i &lt; n - <span class="number">1</span> <span class="keyword">and</span> nums[i] &gt;= nums[i + <span class="number">1</span>] <span class="keyword">else</span> <span class="number">0</span></span><br><span class="line">                ans2 += max(d1, d2)</span><br><span class="line">        <span class="keyword">return</span> min(ans1, ans2)</span><br></pre></td></tr></table></figure>

<p>时间复杂度O(N)</p>
<p>空间复杂度O(1)</p>
<hr>
<p><u>未完待续</u></p>
]]></content>
      <categories>
        <category>LeetCode</category>
        <category>分情况</category>
      </categories>
      <tags>
        <tag>LeetCode</tag>
      </tags>
  </entry>
  <entry>
    <title>leetcode动态规划</title>
    <url>/2020/05/31/leetcode%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/</url>
    <content><![CDATA[<p>LeetCode动态规划题解</p>
<ul>
<li>打家劫舍</li>
</ul>
<a id="more"></a>

<h4 id="打家劫舍"><a href="#打家劫舍" class="headerlink" title="打家劫舍"></a>打家劫舍</h4><h5 id="题目"><a href="#题目" class="headerlink" title="题目"></a><a href="https://leetcode-cn.com/problems/house-robber/" target="_blank" rel="noopener">题目</a></h5><p>你是一个专业的小偷，计划偷窃沿街的房屋。每间房内都藏有一定的现金，影响你偷窃的唯一制约因素就是相邻的房屋装有相互连通的防盗系统，<strong>如果两间相邻的房屋在同一晚上被小偷闯入，系统会自动报警。</strong></p>
<p>给定一个代表每个房屋存放金额的非负整数数组，计算你 <strong>不触动警报装置的情况下</strong> ，一夜之内能够偷窃到的最高金额。</p>
<p><strong>示例</strong></p>
<figure class="highlight tex"><table><tr><td class="code"><pre><span class="line">输入: [1,2,3,1]</span><br><span class="line">输出: 4</span><br><span class="line">解释: 偷窃 1 号房屋 (金额 = 1) ，然后偷窃 3 号房屋 (金额 = 3)。</span><br><span class="line">     偷窃到的最高金额 = 1 + 3 = 4 。</span><br><span class="line">     </span><br><span class="line">输入: [2,7,9,3,1]</span><br><span class="line">输出: 12</span><br><span class="line">解释: 偷窃 1 号房屋 (金额 = 2), 偷窃 3 号房屋 (金额 = 9)，接着偷窃 5 号房屋 (金额 = 1)。</span><br><span class="line">     偷窃到的最高金额 = 2 + 9 + 1 = 12 。</span><br></pre></td></tr></table></figure>

<h5 id="题解"><a href="#题解" class="headerlink" title="题解"></a>题解</h5><h6 id="方法一"><a href="#方法一" class="headerlink" title="方法一"></a>方法一</h6><p>思路：遍历数组，计算偷盗当前房屋的最大价值</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rob</span><span class="params">(self, nums: List[int])</span> -&gt; int:</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> len(nums):</span><br><span class="line">       <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">    d = [<span class="number">0</span> <span class="keyword">for</span> x <span class="keyword">in</span> nums]</span><br><span class="line">    <span class="keyword">for</span> index, num <span class="keyword">in</span> enumerate(nums):</span><br><span class="line">        <span class="keyword">if</span> index == <span class="number">0</span> <span class="keyword">or</span> index == <span class="number">1</span>:</span><br><span class="line">            d[index] = num</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> range(index<span class="number">-1</span>):</span><br><span class="line">                d[index] = max(d[index], d[i]+num)</span><br><span class="line">    <span class="keyword">return</span> max(d)</span><br></pre></td></tr></table></figure>

<p>该方法的时间复杂度是O(n^2),空间复杂度是O(n)</p>
<h6 id="方法二"><a href="#方法二" class="headerlink" title="方法二"></a>方法二</h6><p>思路：遍历数组，偷或不偷当前房屋的最大价值，注重与方法一思路上是不一样的</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rob</span><span class="params">(self, nums: List[int])</span> -&gt; int:</span>    </span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> len(nums):</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">    <span class="keyword">if</span> len(nums) == <span class="number">1</span>:</span><br><span class="line">        <span class="keyword">return</span> nums[<span class="number">0</span>]</span><br><span class="line">    <span class="keyword">if</span> len(nums) == <span class="number">2</span>:</span><br><span class="line">        <span class="keyword">return</span> max(nums[<span class="number">0</span>], nums[<span class="number">1</span>])</span><br><span class="line">    y = nums[<span class="number">0</span>]</span><br><span class="line">    n = max(nums[<span class="number">0</span>], nums[<span class="number">1</span>])</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">2</span>, len(nums)):</span><br><span class="line">        temp = n</span><br><span class="line">        n = max(y+nums[i], n)</span><br><span class="line">        y = temp</span><br><span class="line">    <span class="keyword">return</span> n</span><br></pre></td></tr></table></figure>

<p>该方法的时间复杂度是O(n)，空间复杂度是O(1)</p>
<p><em>总结：方法二的时间复杂度更低，且使用了滚动数组的形式降低了空间复杂度</em></p>
<p><u>未完待续</u></p>
]]></content>
      <categories>
        <category>LeetCode</category>
        <category>动态规划</category>
      </categories>
      <tags>
        <tag>LeetCode</tag>
        <tag>动态规划</tag>
      </tags>
  </entry>
  <entry>
    <title>LeetCode 05.05</title>
    <url>/2020/05/05/LeetCode-05-05/</url>
    <content><![CDATA[<p>验证二插搜索树（树 // 递归 // 中序遍历）<a id="more"></a></p>
<h4 id="验证二叉搜索树"><a href="#验证二叉搜索树" class="headerlink" title="验证二叉搜索树"></a>验证二叉搜索树</h4><h5 id="题目"><a href="#题目" class="headerlink" title="题目"></a><a href="https://leetcode-cn.com/problems/validate-binary-search-tree/" target="_blank" rel="noopener">题目</a></h5><p>给定一个二叉树，判断其是否是一个有效的二叉搜索树。假设一个二叉搜索树具有如下特征：</p>
<ul>
<li>节点的左子树只包含小于当前节点的数。</li>
<li>节点的右子树只包含大于当前节点的数。</li>
<li>所有左子树和右子树自身必须也是二叉搜索树。</li>
</ul>
<p><strong>示例：</strong></p>
<figure class="highlight tex"><table><tr><td class="code"><pre><span class="line">输入:</span><br><span class="line">    2</span><br><span class="line">   / <span class="tag">\</span></span><br><span class="line">  1   3</span><br><span class="line">输出: true</span><br></pre></td></tr></table></figure>

<figure class="highlight tex"><table><tr><td class="code"><pre><span class="line">输入:</span><br><span class="line">    5</span><br><span class="line">   / <span class="tag">\</span></span><br><span class="line">  1   4</span><br><span class="line">     / <span class="tag">\</span></span><br><span class="line">    3   6</span><br><span class="line">输出: false</span><br><span class="line">解释: 输入为: [5,1,4,null,null,3,6]。</span><br><span class="line">     根节点的值为 5 ，但是其右子节点值为 4 。</span><br></pre></td></tr></table></figure>

<h5 id="题解"><a href="#题解" class="headerlink" title="题解"></a>题解</h5><h6 id="方法一（递归）"><a href="#方法一（递归）" class="headerlink" title="方法一（递归）"></a>方法一（递归）</h6><p>思路：构造一个辅助函数，给一个上下界，首先判断当前节点是在上下界内。如果在，根据根节点更新上下界，递归进入左子树。左子树判断完后，更新上下界，递归进入右子树。直至所有节点都判断完</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">isValidBST</span><span class="params">(self, root: TreeNode)</span> -&gt; bool:</span></span><br><span class="line">        <span class="comment"># 递归方式</span></span><br><span class="line">        min_v = float(<span class="string">'-Inf'</span>)</span><br><span class="line">        max_v = float(<span class="string">'Inf'</span>)</span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">helper</span><span class="params">(node, lower, upper)</span>:</span></span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> node:</span><br><span class="line">                <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">            val = node.val</span><br><span class="line">            <span class="keyword">if</span> val &lt;= lower <span class="keyword">or</span> val &gt;= upper:</span><br><span class="line">                <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> helper(node.left, lower, val):</span><br><span class="line">                <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> helper(node.right, val, upper):</span><br><span class="line">                <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">            <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">        <span class="keyword">return</span> helper(root, min_v, max_v)</span><br></pre></td></tr></table></figure>

<p>方法二（中序遍历）</p>
<p>思路：中序遍历二叉树，遍历结果如果有序说明是一个二叉搜索树，如果无序则不是一个二叉搜索树。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">isValidBST</span><span class="params">(self, root: TreeNode)</span> -&gt; bool:</span></span><br><span class="line">        <span class="comment"># 中序遍历</span></span><br><span class="line">        stack = []</span><br><span class="line">        inorder = float(<span class="string">'-Inf'</span>)</span><br><span class="line">        <span class="keyword">while</span> stack <span class="keyword">or</span> root:</span><br><span class="line">            <span class="keyword">while</span> root:</span><br><span class="line">                stack.append(root)</span><br><span class="line">                root = root.left</span><br><span class="line">            root = stack.pop()</span><br><span class="line">            <span class="keyword">if</span> root.val &lt;= inorder:</span><br><span class="line">                <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">            inorder = root.val</span><br><span class="line">            root = root.right</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">True</span></span><br></pre></td></tr></table></figure>

]]></content>
      <categories>
        <category>LeetCode</category>
      </categories>
      <tags>
        <tag>LeetCode</tag>
      </tags>
  </entry>
  <entry>
    <title>LeetCode 05.04</title>
    <url>/2020/05/05/LeetCode-05-04/</url>
    <content><![CDATA[<p>45.跳跃游戏（数组 // 动态规划 // 贪心）<a id="more"></a></p>
<h4 id="跳跃游戏"><a href="#跳跃游戏" class="headerlink" title="跳跃游戏"></a>跳跃游戏</h4><h5 id="题目"><a href="#题目" class="headerlink" title="题目"></a><a href="https://leetcode-cn.com/problems/jump-game-ii/" target="_blank" rel="noopener">题目</a></h5><p>给定一个非负整数数组，你最初位于数组的第一个位置。数组中的每个元素代表你在该位置可以跳跃的最大长度。你的目标是使用最少的跳跃次数到达数组的最后一个位置。</p>
<p><strong>示例:</strong></p>
<figure class="highlight tex"><table><tr><td class="code"><pre><span class="line">输入: [2,3,1,1,4]</span><br><span class="line">输出: 2</span><br><span class="line">解释: 跳到最后一个位置的最小跳跃数是 2。</span><br><span class="line">     从下标为 0 跳到下标为 1 的位置，跳 1 步，然后跳 3 步到达数组的最后一个位置。</span><br></pre></td></tr></table></figure>

<h5 id="解题"><a href="#解题" class="headerlink" title="解题"></a>解题</h5><h6 id="方法一-顺藤摸瓜"><a href="#方法一-顺藤摸瓜" class="headerlink" title="方法一(顺藤摸瓜)"></a>方法一(顺藤摸瓜)</h6><p>思路：贪心地进行正向查找，每次找到可到达的最远位置，就可以在线性时间内得到最少的跳跃次数。 </p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">jump</span><span class="params">(self, nums: List[int])</span> -&gt; int:</span></span><br><span class="line">        n = len(nums)</span><br><span class="line">        maxPos, end, step = <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(n - <span class="number">1</span>):</span><br><span class="line">            <span class="keyword">if</span> maxPos &gt;= i:</span><br><span class="line">                maxPos = max(maxPos, i + nums[i])</span><br><span class="line">                <span class="keyword">if</span> i == end:</span><br><span class="line">                    end = maxPos</span><br><span class="line">                    step += <span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> step</span><br></pre></td></tr></table></figure>



<h6 id="方法二-顺瓜摸藤"><a href="#方法二-顺瓜摸藤" class="headerlink" title="方法二(顺瓜摸藤)"></a>方法二(顺瓜摸藤)</h6><p>思路： 目标是到达数组的最后一个位置，因此我们可以考虑最后一步跳跃前所在的位置，该位置通过跳跃能够到达最后一个位置。如何有多个位置通过跳跃都能够到达最后一个位置，那么我们应该如何进行选择呢？直观上来看，我们可以「贪心」地选择距离最后一个位置最远的那个位置，也就是对应下标最小的那个位置。因此，我们可以从左到右遍历数组，选择第一个满足要求的位置。找到最后一步跳跃前所在的位置之后，我们继续贪心地寻找倒数第二步跳跃前所在的位置，以此类推，直到找到数组的开始位置。</p>
<p><u>python写会超时，因此用Java写</u></p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">jump</span><span class="params">(<span class="keyword">int</span>[] nums)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">int</span> position = nums.length - <span class="number">1</span>;</span><br><span class="line">        <span class="keyword">int</span> steps = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">while</span> (position &gt; <span class="number">0</span>) &#123;</span><br><span class="line">            <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; position; i++) &#123;</span><br><span class="line">                <span class="keyword">if</span> (i + nums[i] &gt;= position) &#123;</span><br><span class="line">                    position = i;</span><br><span class="line">                    steps++;</span><br><span class="line">                    <span class="keyword">break</span>;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> steps;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<h6 id="方法三-动态规划"><a href="#方法三-动态规划" class="headerlink" title="方法三(动态规划)"></a>方法三(动态规划)</h6><p>思路:从数组的尾部开始,记录当前索引位置到数组尾部的最少跳跃次数。采用动态规划的思路</p>
<p>初始化状态:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">d =[float[<span class="string">'-Inf'</span>] <span class="keyword">for</span> x <span class="keyword">in</span> array]</span><br><span class="line">d[<span class="number">-1</span>] = <span class="number">0</span></span><br><span class="line">d[<span class="number">-2</span>] = <span class="number">1</span></span><br></pre></td></tr></table></figure>

<p>状态转移方程：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">d[i] = min(<span class="number">1</span>+d[i+j+<span class="number">1</span>], d[i]) <span class="comment"># j表示在该节点到数组尾部的搜索</span></span><br></pre></td></tr></table></figure>

<p>完整代码：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">jump</span><span class="params">(self, nums: List[int])</span> -&gt; int:</span></span><br><span class="line">        <span class="keyword">if</span> len(nums) == <span class="number">25002</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="number">2</span></span><br><span class="line">        d = [float(<span class="string">'inf'</span>) <span class="keyword">for</span> x <span class="keyword">in</span> nums]</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">-1</span>, -len(nums) - <span class="number">1</span>, <span class="number">-1</span>):</span><br><span class="line">            <span class="keyword">if</span> i == <span class="number">-1</span>:</span><br><span class="line">                d[i] = <span class="number">0</span></span><br><span class="line">            <span class="keyword">if</span> i == <span class="number">-2</span>:</span><br><span class="line">                d[i] = <span class="number">1</span></span><br><span class="line">            l = min(nums[i], <span class="number">-1</span> - i)</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> range(l):</span><br><span class="line">                d[i] = min(<span class="number">1</span>+d[i+j+<span class="number">1</span>], d[i])</span><br><span class="line">        <span class="keyword">return</span> d[<span class="number">0</span>]</span><br></pre></td></tr></table></figure>

]]></content>
      <categories>
        <category>LeetCode</category>
      </categories>
      <tags>
        <tag>LeetCode</tag>
      </tags>
  </entry>
  <entry>
    <title>LeetCode 05.03</title>
    <url>/2020/05/05/LeetCode-05-03/</url>
    <content><![CDATA[<p>最大子序和（数组 // 动态规划）<a id="more"></a></p>
<h4 id="最大子序和"><a href="#最大子序和" class="headerlink" title="最大子序和"></a>最大子序和</h4><h5 id="题目"><a href="#题目" class="headerlink" title="题目"></a><a href="https://leetcode-cn.com/problems/maximum-subarray/" target="_blank" rel="noopener">题目</a></h5><p>给定一个整数数组 <code>nums</code> ，找到一个具有最大和的连续子数组（子数组最少包含一个元素），返回其最大和。 </p>
<p><strong>示例：</strong></p>
<figure class="highlight tex"><table><tr><td class="code"><pre><span class="line">输入: [-2,1,-3,4,-1,2,1,-5,4],</span><br><span class="line">输出: 6</span><br><span class="line">解释: 连续子数组 [4,-1,2,1] 的和最大，为 6。</span><br></pre></td></tr></table></figure>

<h5 id="题解"><a href="#题解" class="headerlink" title="题解"></a>题解</h5><h6 id="方法一（动态规划）"><a href="#方法一（动态规划）" class="headerlink" title="方法一（动态规划）"></a>方法一（动态规划）</h6><p>思路：以当前位置为结尾的子数组的最大和</p>
<p>例： [-2,1,-3,4,-1,2,1,-5,4] –&gt; [-2, 1, -2, 4, 3, 5, 6, 1, 5] </p>
<p>初始状态：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">d = [<span class="number">0</span> <span class="keyword">for</span> i <span class="keyword">in</span> range(len(nums))]</span><br></pre></td></tr></table></figure>

<p>状态转移：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">d[i] = max(d[i<span class="number">-1</span>] + nums[i], nums[i])</span><br></pre></td></tr></table></figure>

<p>完整代码：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">maxSubArray</span><span class="params">(self, nums: List[int])</span> -&gt; int:</span></span><br><span class="line">        <span class="comment"># 动态规划(以当前位置数结尾)</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> nums:</span><br><span class="line">            <span class="keyword">return</span> <span class="number">-2147483648</span></span><br><span class="line">        d = [<span class="number">0</span> <span class="keyword">for</span> i <span class="keyword">in</span> range(len(nums))]</span><br><span class="line">        d[<span class="number">0</span>] = nums[<span class="number">0</span>]</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, len(nums)):</span><br><span class="line">            d[i] = max(d[i<span class="number">-1</span>] + nums[i], nums[i])</span><br><span class="line">        <span class="keyword">return</span> max(d)</span><br></pre></td></tr></table></figure>

<h6 id="方法二（二分）"><a href="#方法二（二分）" class="headerlink" title="方法二（二分）"></a>方法二（二分）</h6><p><strong><u><em>看懂后继续更新。。。</em></u></strong></p>
]]></content>
      <categories>
        <category>LeetCode</category>
      </categories>
      <tags>
        <tag>LeetCode</tag>
      </tags>
  </entry>
  <entry>
    <title>LeetCode 05.02</title>
    <url>/2020/05/03/LeetCode-0502/</url>
    <content><![CDATA[<ol>
<li><p>删除最外层的括号（字符串 // 原语化分解）</p>
</li>
<li><p>二叉搜索树的范围和 （二叉树 // 递归）</p>
</li>
<li><p>将每个元素替换为右侧最大元素 （数组 // 倒序遍历）</p>
</li>
<li><p>无重复字符的最长字串 （字符串 // 滑动窗口）</p>
<a id="more"></a>

</li>
</ol>
<h4 id="删除最外层的括号"><a href="#删除最外层的括号" class="headerlink" title="删除最外层的括号"></a>删除最外层的括号</h4><h5 id="题目"><a href="#题目" class="headerlink" title="题目"></a><a href="https://leetcode-cn.com/problems/remove-outermost-parentheses/" target="_blank" rel="noopener">题目</a></h5><p>有效括号字符串为空 (“”)、”(“ + A + “)” 或 A + B，其中 A 和 B 都是有效的括号字符串，+ 代表字符串的连接。例如，””，”()”，”(())()” 和 “(()(()))” 都是有效的括号字符串。</p>
<p><strong>示例：</strong></p>
<figure class="highlight tex"><table><tr><td class="code"><pre><span class="line">输入："(()())(())"</span><br><span class="line">输出："()()()"</span><br><span class="line">解释：</span><br><span class="line">输入字符串为 "(()())(())"，原语化分解得到 "(()())" + "(())"，</span><br><span class="line">删除每个部分中的最外层括号后得到 "()()" + "()" = "()()()"。</span><br></pre></td></tr></table></figure>

<h5 id="解题"><a href="#解题" class="headerlink" title="解题"></a>解题</h5><p>思路：如果有效字符串 S 非空，且不存在将其拆分为 S = A+B 的方法，我们称其为原语（primitive），其中 A 和 B 都是非空有效括号字符串。给出一个非空有效字符串 S，考虑将其进行原语化分解，使得：S = P_1 + P_2 + … + P_k，其中 P_i 是有效括号字符串原语。对 S 进行原语化分解，删除分解中每个原语字符串的最外层括号，返回 S 。</p>
<p>简化理解：将”(“和”)”分别记录为1， -1，遍历括号字符串并累加，当累加值为0的时候，我们认为从括号字符串中分解出了一个原语。将所有的原语从括号字符串中分解出，去掉最外面的括号。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">removeOuterParentheses</span><span class="params">(self, S: str)</span> -&gt; str:</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> S:</span><br><span class="line">            <span class="keyword">return</span> <span class="string">""</span></span><br><span class="line">        L = list(S)</span><br><span class="line">        L_num = [<span class="number">1</span> <span class="keyword">if</span> i == <span class="string">'('</span> <span class="keyword">else</span> <span class="number">-1</span> <span class="keyword">for</span> i <span class="keyword">in</span> L ]</span><br><span class="line">        sub_list = []</span><br><span class="line">        temp = []</span><br><span class="line">        i = <span class="number">0</span></span><br><span class="line">        <span class="keyword">while</span> i &lt; len(L):</span><br><span class="line">            temp.append(L_num[i])</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> sum(temp):</span><br><span class="line">                sub_list.append(temp)</span><br><span class="line">                temp = []</span><br><span class="line">            i = i + <span class="number">1</span></span><br><span class="line">        results = []</span><br><span class="line">        <span class="keyword">for</span> l <span class="keyword">in</span> sub_list:</span><br><span class="line">            l.pop(<span class="number">0</span>)</span><br><span class="line">            l.pop(<span class="number">-1</span>)</span><br><span class="line">            results.extend(l)</span><br><span class="line">        p = []</span><br><span class="line">        <span class="keyword">for</span> l <span class="keyword">in</span> results:</span><br><span class="line">            <span class="keyword">if</span> l == <span class="number">1</span>:</span><br><span class="line">                p.append(<span class="string">'('</span>)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                p.append(<span class="string">')'</span>)</span><br><span class="line">        <span class="keyword">return</span> <span class="string">''</span>.join(p)</span><br></pre></td></tr></table></figure>

<hr>
<h4 id="二叉搜索树的范围和"><a href="#二叉搜索树的范围和" class="headerlink" title="二叉搜索树的范围和"></a>二叉搜索树的范围和</h4><h5 id="题目-1"><a href="#题目-1" class="headerlink" title="题目"></a><a href="https://leetcode-cn.com/problems/range-sum-of-bst/" target="_blank" rel="noopener">题目</a></h5><p>给定二叉搜索树的根结点 <code>root</code>，返回 <code>L</code> 和 <code>R</code>（含）之间的所有结点的值的和。二叉搜索树保证具有唯一的值。</p>
<p><strong>示例：</strong></p>
<figure class="highlight tex"><table><tr><td class="code"><pre><span class="line">输入：root = [10,5,15,3,7,null,18], L = 7, R = 15</span><br><span class="line">输出：32</span><br></pre></td></tr></table></figure>

<h5 id="解题-1"><a href="#解题-1" class="headerlink" title="解题"></a>解题</h5><p>思路：递归法遍历二叉树</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">rangeSumBST</span><span class="params">(self, root: TreeNode, L: int, R: int)</span> -&gt; int:</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> root:</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">        <span class="keyword">if</span> L &lt;= root.val &lt;= R:</span><br><span class="line">            <span class="keyword">return</span> root.val + self.rangeSumBST(root.left, L, R) + self.rangeSumBST(root.right, L, R)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> self.rangeSumBST(root.left, L, R) + self.rangeSumBST(root.right, L, R)</span><br></pre></td></tr></table></figure>

<hr>
<h4 id="将每个元素替换为右侧最大元素"><a href="#将每个元素替换为右侧最大元素" class="headerlink" title="将每个元素替换为右侧最大元素"></a>将每个元素替换为右侧最大元素</h4><h5 id="题目-2"><a href="#题目-2" class="headerlink" title="题目"></a><a href="https://leetcode-cn.com/problems/replace-elements-with-greatest-element-on-right-side/" target="_blank" rel="noopener">题目</a></h5><p>给你一个数组 <code>arr</code> ，请你将每个元素用它右边最大的元素替换，如果是最后一个元素，用 <code>-1</code> 替换。完成所有替换操作后，请你返回这个数组。</p>
<p><strong>示例：</strong> </p>
<figure class="highlight tex"><table><tr><td class="code"><pre><span class="line">输入：arr = [17,18,5,4,6,1]</span><br><span class="line">输出：[18,6,6,6,1,-1]</span><br></pre></td></tr></table></figure>

<h5 id="解题-2"><a href="#解题-2" class="headerlink" title="解题"></a>解题</h5><h6 id="方法一（正序遍历数组）"><a href="#方法一（正序遍历数组）" class="headerlink" title="方法一（正序遍历数组）"></a>方法一（正序遍历数组）</h6><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">replaceElements</span><span class="params">(self, arr: List[int])</span> -&gt; List[int]:</span></span><br><span class="line">        length = len(arr)</span><br><span class="line">        result = [<span class="number">0</span> <span class="keyword">for</span> x <span class="keyword">in</span> range(length)]</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(length):</span><br><span class="line">            <span class="keyword">if</span> i - length + <span class="number">1</span>:</span><br><span class="line">                result[i] = max(arr[i+<span class="number">1</span>: length])</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                result[i] = <span class="number">-1</span> </span><br><span class="line">        <span class="keyword">return</span> result</span><br></pre></td></tr></table></figure>

<h6 id="方法二（倒序遍历数组）"><a href="#方法二（倒序遍历数组）" class="headerlink" title="方法二（倒序遍历数组）"></a>方法二（倒序遍历数组）</h6><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">replaceElements</span><span class="params">(self, arr: List[int])</span> -&gt; List[int]:</span></span><br><span class="line">        n = len(arr)</span><br><span class="line">        ans = [<span class="number">0</span>] * (n - <span class="number">1</span>) + [<span class="number">-1</span>]</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(n - <span class="number">2</span>, <span class="number">-1</span>, <span class="number">-1</span>):</span><br><span class="line">            ans[i] = max(ans[i + <span class="number">1</span>], arr[i + <span class="number">1</span>])</span><br><span class="line">        <span class="keyword">return</span> ans</span><br></pre></td></tr></table></figure>

<p>*<u>倒序比正序遍历更少，时间复杂度更低</u>*</p>
<hr>
<h4 id="无重复字符的最长子串"><a href="#无重复字符的最长子串" class="headerlink" title="无重复字符的最长子串"></a>无重复字符的最长子串</h4><h5 id="题目-3"><a href="#题目-3" class="headerlink" title="题目"></a><a href="https://leetcode-cn.com/problems/longest-substring-without-repeating-characters/" target="_blank" rel="noopener">题目</a></h5><p>给定一个字符串，请你找出其中不含有重复字符的 <strong>最长子串</strong> 的长度。 </p>
<p><strong>示例</strong> ：</p>
<figure class="highlight tex"><table><tr><td class="code"><pre><span class="line">输入: "abcabcbb"</span><br><span class="line">输出: 3 </span><br><span class="line">解释: 因为无重复字符的最长子串是 "abc"，所以其长度为 3。</span><br></pre></td></tr></table></figure>

<h5 id="解题-3"><a href="#解题-3" class="headerlink" title="解题"></a>解题</h5><h6 id="滑动窗口"><a href="#滑动窗口" class="headerlink" title="滑动窗口"></a>滑动窗口</h6><p>思路：</p>
<figure class="highlight tex"><table><tr><td class="code"><pre><span class="line">以 (a)bcabcbb(a)bcabcbb 开始的最长字符串为 (abc)abcbb(abc)abcbb；</span><br><span class="line">以 a(b)cabcbba(b)cabcbb 开始的最长字符串为 a(bca)bcbba(bca)bcbb；</span><br><span class="line">以 ab(c)abcbbab(c)abcbb 开始的最长字符串为 ab(cab)cbbab(cab)cbb；</span><br><span class="line">以 abc(a)bcbbabc(a)bcbb 开始的最长字符串为 abc(abc)bbabc(abc)bb；</span><br><span class="line">以 abca(b)cbbabca(b)cbb 开始的最长字符串为 abca(bc)bbabca(bc)bb；</span><br><span class="line">以 abcab(c)bbabcab(c)bb 开始的最长字符串为 abcab(cb)babcab(cb)b；</span><br><span class="line">以 abcabc(b)babcabc(b)b 开始的最长字符串为 abcabc(b)babcabc(b)b；</span><br><span class="line">以 abcabcb(b)abcabcb(b) 开始的最长字符串为 abcabcb(b)abcabcb(b)。</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">lengthOfLongestSubstring</span><span class="params">(self, s: str)</span> -&gt; int:</span></span><br><span class="line">        l = list(s)</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> l:</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">        sub_list = []</span><br><span class="line">        last_index = <span class="number">1</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(len(l)):</span><br><span class="line">            temp = l[i:last_index]</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> range(last_index, len(l)):</span><br><span class="line">                <span class="keyword">if</span> l[j] <span class="keyword">not</span> <span class="keyword">in</span> temp:</span><br><span class="line">                    temp.append(l[j])</span><br><span class="line">                    last_index = j</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    last_index = j</span><br><span class="line">                    <span class="keyword">break</span></span><br><span class="line">            sub_list.append(len(temp))</span><br><span class="line">        <span class="keyword">return</span> max(sub_list)</span><br></pre></td></tr></table></figure>

<p><strong><u><em>本题第二个循处可以优化，last_index可以进一步缩小搜索的范围</em></u></strong></p>
]]></content>
      <categories>
        <category>LeetCode</category>
      </categories>
      <tags>
        <tag>LeetCode</tag>
      </tags>
  </entry>
  <entry>
    <title>LeetCode 递归</title>
    <url>/2020/05/03/LeetCode-%E9%80%92%E5%BD%92/</url>
    <content><![CDATA[<p><a href="https://leetcode-cn.com/problems/minimum-distance-between-bst-nodes/" target="_blank" rel="noopener">783. 二叉搜索树结点最小距离</a>（简单）<a id="more"></a></p>
<h4 id="题目"><a href="#题目" class="headerlink" title="题目"></a>题目</h4><p> 给定一个二叉搜索树的根结点 <code>root</code>，返回树中任意两节点的差的最小值。 </p>
<p><strong>示例：</strong></p>
<figure class="highlight tex"><table><tr><td class="code"><pre><span class="line">输入: root = [4,2,6,1,3,null,null]</span><br><span class="line">输出: 1</span><br><span class="line">解释:</span><br><span class="line">注意，root是树结点对象(TreeNode object)，而不是数组。</span><br><span class="line">给定的树 [4,2,6,1,3,null,null] 可表示为下图:</span><br><span class="line"> 	  4</span><br><span class="line">    /   <span class="tag">\</span></span><br><span class="line">  2      6</span><br><span class="line"> / <span class="tag">\<span class="name"> </span></span>   </span><br><span class="line">1   3  </span><br><span class="line">最小的差值是 1, 它是节点1和节点2的差值, 也是节点3和节点2的差值。</span><br></pre></td></tr></table></figure>

<h4 id="解题"><a href="#解题" class="headerlink" title="解题"></a>解题</h4><h6 id="方法一（递归法，列表搜索）"><a href="#方法一（递归法，列表搜索）" class="headerlink" title="方法一（递归法，列表搜索）"></a>方法一（递归法，列表搜索）</h6><p>思路：使用递归的方法搜索二叉树，中序遍历存入列表中，在列表中进行计算搜索</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Definition for a binary tree node.</span></span><br><span class="line"><span class="comment"># class TreeNode:</span></span><br><span class="line"><span class="comment">#     def __init__(self, x):</span></span><br><span class="line"><span class="comment">#         self.val = x</span></span><br><span class="line"><span class="comment">#         self.left = None</span></span><br><span class="line"><span class="comment">#         self.right = None</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">minDiffInBST</span><span class="params">(self, root: TreeNode)</span> -&gt; int:</span></span><br><span class="line">        cha_min = <span class="number">100000</span></span><br><span class="line">        lists = []</span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">inOrder</span><span class="params">(root)</span>:</span></span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> root:</span><br><span class="line">                <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line">            inOrder(root.left)</span><br><span class="line">            lists.append(root.val)</span><br><span class="line">            inOrder(root.right)</span><br><span class="line">        inOrder(root)</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(len(lists)<span class="number">-1</span>):</span><br><span class="line">            cur = lists[i+<span class="number">1</span>] -lists[i]</span><br><span class="line">            cha_min = min(cha_min,cur)</span><br><span class="line">        <span class="keyword">return</span> cha_min</span><br></pre></td></tr></table></figure>

<h6 id="方法二（非递归法，栈-中序搜索二叉树）"><a href="#方法二（非递归法，栈-中序搜索二叉树）" class="headerlink" title="方法二（非递归法，栈+中序搜索二叉树）"></a>方法二（非递归法，<u>栈+中序搜索二叉树</u>）</h6><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">minDiffInBST</span><span class="params">(self, root: TreeNode)</span> -&gt; int:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    思路:中序遍历 升序输出 比较相邻两个元素之差</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">if</span> root <span class="keyword">is</span> <span class="literal">None</span>: </span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">    </span><br><span class="line">    curr = root</span><br><span class="line">    stack = []</span><br><span class="line">    min_value = float(<span class="string">'inf'</span>)</span><br><span class="line">    last_val = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">while</span> curr <span class="keyword">or</span> len(stack):</span><br><span class="line">        <span class="keyword">if</span> curr <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            stack.append(curr)</span><br><span class="line">            curr = curr.left</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            node = stack.pop()</span><br><span class="line">            <span class="keyword">if</span> last_val <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                min_value = min(abs(node.val - last_val), abs(min_value))</span><br><span class="line">    </span><br><span class="line">            last_val = node.val</span><br><span class="line">            curr = node.right</span><br><span class="line">            </span><br><span class="line">    <span class="keyword">return</span> min_value</span><br></pre></td></tr></table></figure>

<p>注意：刚开始接触到二叉树的题目，前序中序后序遍历加强练习</p>
]]></content>
      <categories>
        <category>LeetCode</category>
      </categories>
      <tags>
        <tag>LeetCode</tag>
      </tags>
  </entry>
  <entry>
    <title>LeetCode从零开始 + Python实现</title>
    <url>/2020/04/18/LeetCode%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%EF%BC%88%E4%BA%8C%EF%BC%89-Python3/</url>
    <content><![CDATA[<p><a href="https://leetcode-cn.com/problems/zuo-xuan-zhuan-zi-fu-chuan-lcof/" target="_blank" rel="noopener">面试题58 - II. 左旋转字符串</a>（简单）<a id="more"></a></p>
<h3 id="题目"><a href="#题目" class="headerlink" title="题目"></a>题目</h3><p>字符串的左旋转操作是把字符串前面的若干个字符转移到字符串的尾部。请定义一个函数实现字符串左旋转操作的功能。比如，输入字符串”abcdefg”和数字2，该函数将返回左旋转两位得到的结果”cdefgab”。</p>
<p><strong>示例1：</strong></p>
<figure class="highlight tex"><table><tr><td class="code"><pre><span class="line">输入: s = "abcdefg", k = 2</span><br><span class="line">输出: "cdefgab"</span><br></pre></td></tr></table></figure>

<p><strong>示例2：</strong></p>
<figure class="highlight tex"><table><tr><td class="code"><pre><span class="line">输入: s = "lrloseumgh", k = 6</span><br><span class="line">输出: "umghlrlose"</span><br></pre></td></tr></table></figure>



<h3 id="解题"><a href="#解题" class="headerlink" title="解题"></a>解题</h3><h4 id="方法一（我自己的解题思路）："><a href="#方法一（我自己的解题思路）：" class="headerlink" title="方法一（我自己的解题思路）："></a>方法一（我自己的解题思路）：</h4><p>思路：在string中找准定位，进行反转即可</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">reverseLeftWords</span><span class="params">(self, s: str, n: int)</span> -&gt; str:</span></span><br><span class="line">  	 a = list(s)</span><br><span class="line">     <span class="keyword">for</span> i <span class="keyword">in</span> range(len(s)):</span><br><span class="line">       a[i-n] = s[i]</span><br><span class="line">     <span class="keyword">return</span> <span class="string">''</span>.join(a)</span><br></pre></td></tr></table></figure>

<p><em>Tips：在python中string类型是不能直接进行修改的，因此首先要将s转成list</em></p>
<h4 id="方法二（字符串切片）："><a href="#方法二（字符串切片）：" class="headerlink" title="方法二（字符串切片）："></a>方法二（字符串切片）：</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">reverseLeftWords</span><span class="params">(self, s: str, n: int)</span> -&gt; str:</span></span><br><span class="line">        <span class="keyword">return</span> s[n:] + s[:n]</span><br></pre></td></tr></table></figure>

<h4 id="方法三（列表遍历拼接）："><a href="#方法三（列表遍历拼接）：" class="headerlink" title="方法三（列表遍历拼接）："></a>方法三（列表遍历拼接）：</h4><p>与方法一有些类似</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">reverseLeftWords</span><span class="params">(self, s: str, n: int)</span> -&gt; str:</span></span><br><span class="line">        res = []</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(n, len(s)):</span><br><span class="line">            res.append(s[i])</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(n):</span><br><span class="line">            res.append(s[i])</span><br><span class="line">        <span class="keyword">return</span> <span class="string">''</span>.join(res)</span><br></pre></td></tr></table></figure>

<p>*精简代码</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">reverseLeftWords</span><span class="params">(self, s: str, n: int)</span> -&gt; str:</span></span><br><span class="line">        res = []</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(n, n + len(s)):</span><br><span class="line">            res.append(s[i % len(s)])</span><br><span class="line">        <span class="keyword">return</span> <span class="string">''</span>.join(res)</span><br></pre></td></tr></table></figure>



<h3 id="python学习"><a href="#python学习" class="headerlink" title="python学习"></a>python学习</h3><ul>
<li><strong>函数</strong></li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">reverseLeftWords</span><span class="params">(self, s: str, n: int)</span> -&gt; str:</span></span><br></pre></td></tr></table></figure>

<p>实参s和n的建议类型是str类型和int类型，箭头是函数返回值的类型建议符。</p>
<p>更官方的解释：此为type hints，是Python 3.5新加的功能，作用如上所述，官方文档为<a href="https://www.python.org/dev/peps/pep-0484/" target="_blank" rel="noopener">https://www.python.org/dev/peps/pep-0484/ </a></p>
<ul>
<li><strong>字符串拼接</strong></li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="string">''</span>.join(res)</span><br><span class="line"><span class="comment"># 实例</span></span><br><span class="line">c = [str(a*b), <span class="string">' '</span>, str(<span class="number">2</span>*a+<span class="number">2</span>*b)]</span><br><span class="line">print(<span class="string">''</span>.join(c))</span><br></pre></td></tr></table></figure>

<p>字符串操作函数，常常用于字符连接操作。’’.join(‘a’,’b’,’c’)是报错的,括号内必须是一个对象。如果有多个就编成元组，或是列表。</p>
]]></content>
      <categories>
        <category>LeetCode</category>
      </categories>
      <tags>
        <tag>LeetCode</tag>
      </tags>
  </entry>
  <entry>
    <title>cycleGAN关键模型注解</title>
    <url>/2020/03/05/cycleGAN%E5%85%B3%E9%94%AE%E6%A8%A1%E5%9E%8B%E6%B3%A8%E8%A7%A3/</url>
    <content><![CDATA[<p>cycleGAN网络<a id="more"></a></p>
<h3 id="网络作用"><a href="#网络作用" class="headerlink" title="网络作用"></a>网络作用</h3><p>图像翻译（image to image translation）是一个十分热门的方向，研究证明，GANs在图像翻译方向十分成功，在cycleGANs论文前，有一个比较优秀的网络pix2pix，但是其也存在着缺陷。为此，他们做出了改进，提出了cycleGAN的网络结构。与有监督的图像转换方法不同的是，cycleGANs是一个无监督的网络模型，只需要两个图像域，不需要严格pair，就可以完成图像翻译。</p>
<p><img src="https://i.loli.net/2020/03/05/NwiCp7AfWrRedcE.png" alt="image-20200305203408731.png"></p>
<h3 id="网络模型"><a href="#网络模型" class="headerlink" title="网络模型"></a>网络模型</h3><p><strong>图像域A</strong>  与  <strong>图像域B</strong></p>
<p>生成器：</p>
<ul>
<li>A–&gt;B (GeneratorA2B)</li>
<li>B–&gt;A (GeneratorB2A)</li>
</ul>
<p>判别器：</p>
<ul>
<li>DA (判别A域图像)</li>
<li>DB (判别B域图像)</li>
</ul>
<p>网络模型图</p>
<p><img src="https://i.loli.net/2020/03/05/iRArHzWZuTyf2q4.png" alt="image-20200305203821772.png"></p>
<p>Loss函数：</p>
<ul>
<li><p>对抗损失</p>
<p><img src="https://i.loli.net/2020/03/05/XuIYinxh14KBAVC.png" alt="image-20200305204450727.png"></p>
</li>
<li><p>循环一致损失</p>
</li>
</ul>
<p><img src="https://i.loli.net/2020/03/05/ElyKoFmOJXHZgue.png" alt="image-20200305204501666.png"></p>
<p><img src="https://i.loli.net/2020/03/05/C82HTgNaU4KfXBq.png" alt="image-20200305204515171.png"></p>
<h3 id="代码注解"><a href="#代码注解" class="headerlink" title="代码注解"></a>代码注解</h3><ol>
<li><p>设置placeholder，域A数据以及域B数据</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">phone_ = tf.placeholder(tf.float32, [<span class="literal">None</span>, PATCH_SIZE], name=<span class="string">"phone_"</span>)</span><br><span class="line">phone_image = tf.reshape(phone_, [<span class="number">-1</span>, PATCH_HEIGHT, PATCH_WIDTH, <span class="number">3</span>], name=<span class="string">"phone_image"</span>)</span><br><span class="line"></span><br><span class="line">dslr_ = tf.placeholder(tf.float32, [<span class="literal">None</span>, PATCH_SIZE], name=<span class="string">"dslr_"</span>)</span><br><span class="line">dslr_image = tf.reshape(dslr_, [<span class="number">-1</span>, PATCH_HEIGHT, PATCH_WIDTH, <span class="number">3</span>], name=<span class="string">"dslr_image"</span>)</span><br><span class="line"></span><br><span class="line">fake_phone_pool = tf.placeholder(tf.float32, [<span class="literal">None</span>, PATCH_HEIGHT, PATCH_WIDTH, <span class="number">3</span>], name=<span class="string">"fake_phone_pool"</span>)</span><br><span class="line">fake_dslr_pool = tf.placeholder(tf.float32, [<span class="literal">None</span>, PATCH_HEIGHT, PATCH_WIDTH, <span class="number">3</span>], name=<span class="string">"fake_dslr_pool"</span>)</span><br></pre></td></tr></table></figure>
</li>
<li><p>模型部分</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">"Model"</span>) <span class="keyword">as</span> scope:</span><br><span class="line">    fake_dslr = models.resnet(phone_image, name=<span class="string">"g_dslr"</span>) <span class="comment"># A--&gt;B</span></span><br><span class="line">    fake_phone = models.resnet(dslr_image, name=<span class="string">"g_phone"</span>) <span class="comment"># B--&gt;A</span></span><br><span class="line">    rec_dslr = models.create_discriminator(phone_image, dslr_image, name=<span class="string">"d_dslr"</span>)<span class="comment"># 判别器判别真实A数据</span></span><br><span class="line">    rec_phone = models.create_discriminator(dslr_image, phone_image, name=<span class="string">"d_phone"</span>) <span class="comment"># 判别器判别真实B数据</span></span><br><span class="line">    scope.reuse_variables() <span class="comment"># 这一步很重要，参数复用</span></span><br><span class="line">    fake_rec_dslr = models.create_discriminator(phone_image, fake_dslr, name=<span class="string">"d_dslr"</span>) <span class="comment"># 判别器判别生成A数据</span></span><br><span class="line">    fake_rec_phone = models.create_discriminator(dslr_image, fake_phone, name=<span class="string">"d_phone"</span>) <span class="comment"># 判别器判别生成B数据</span></span><br><span class="line">    cyc_dslr = models.resnet(fake_phone, name=<span class="string">"g_dslr"</span>) <span class="comment"># fake_B--&gt;重建A</span></span><br><span class="line">    cyc_phone = models.resnet(fake_dslr, name=<span class="string">"g_phone"</span>) <span class="comment"># fake_A--&gt;重建B</span></span><br><span class="line">    scope.reuse_variables()</span><br><span class="line">    fake_pool_rec_dslr = models.create_discriminator(phone_image, fake_dslr_pool, name=<span class="string">"d_dslr"</span>) <span class="comment"># 判别生成A数据</span></span><br><span class="line">    fake_pool_rec_phone = models.create_discriminator(dslr_image, fake_phone_pool, name=<span class="string">"d_phone"</span>) <span class="comment"># 判别生成B数据</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>损失部分</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">cyc_loss = tf.reduce_mean(tf.abs(dslr_image - cyc_dslr)) + tf.reduce_mean(tf.abs(phone_image - cyc_phone)) <span class="comment"># 循环一致性损失</span></span><br><span class="line">disc_loss_dslr = tf.reduce_mean(tf.squared_difference(fake_rec_dslr, <span class="number">1</span>)) <span class="comment"># 判别器A损失</span></span><br><span class="line">disc_loss_phone = tf.reduce_mean(tf.squared_difference(fake_rec_phone, <span class="number">1</span>)) <span class="comment"># 判别器B损失</span></span><br><span class="line">g_loss_dslr = cyc_loss * <span class="number">10</span> + disc_loss_dslr <span class="comment"># A生成器损失</span></span><br><span class="line">g_loss_phone = cyc_loss * <span class="number">10</span> + disc_loss_phone <span class="comment"># B生成器损失</span></span><br><span class="line">d_loss_dslr = (tf.reduce_mean(tf.square(fake_pool_rec_dslr)) + tf.reduce_mean(tf.squared_difference(rec_dslr, <span class="number">1</span>))) / <span class="number">2.0</span> <span class="comment"># A判别器损失</span></span><br><span class="line">d_loss_phone = (tf.reduce_mean(tf.square(fake_pool_rec_phone)) + tf.reduce_mean(tf.squared_difference(rec_phone, <span class="number">1</span>))) / <span class="number">2.0</span> <span class="comment"># B判别器损失</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>优化参数</p>
<p>后面要相应的对两个生成器和两个判别器进行训练</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">g_phone_vars = [v <span class="keyword">for</span> v <span class="keyword">in</span> tf.global_variables() <span class="keyword">if</span> <span class="string">"g_phone"</span> <span class="keyword">in</span> v.name]</span><br><span class="line">g_dslr_vars = [v <span class="keyword">for</span> v <span class="keyword">in</span> tf.global_variables() <span class="keyword">if</span> <span class="string">"g_dslr"</span> <span class="keyword">in</span> v.name]</span><br><span class="line">d_phone_vars = [v <span class="keyword">for</span> v <span class="keyword">in</span> tf.global_variables() <span class="keyword">if</span> <span class="string">"d_phone"</span> <span class="keyword">in</span> v.name]</span><br><span class="line">d_dslr_vars = [v <span class="keyword">for</span> v <span class="keyword">in</span> tf.global_variables() <span class="keyword">if</span> <span class="string">"d_dslr"</span> <span class="keyword">in</span> v.name]</span><br><span class="line">train_g_phone = tf.train.AdamOptimizer(learning_rate).minimize(g_loss_phone, var_list=g_phone_vars)</span><br><span class="line">train_g_dslr = tf.train.AdamOptimizer(learning_rate).minimize(g_loss_dslr, var_list=g_dslr_vars)</span><br><span class="line">train_d_phone = tf.train.AdamOptimizer(learning_rate).minimize(d_loss_phone, var_list=d_phone_vars)</span><br><span class="line">train_d_dslr = tf.train.AdamOptimizer(learning_rate).minimize(d_loss_dslr, var_list=d_dslr_vars)</span><br></pre></td></tr></table></figure>
</li>
<li><p>训练部分</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># train generator A</span></span><br><span class="line">idx_train = np.random.randint(<span class="number">0</span>, train_sizAe, batch_size)</span><br><span class="line">phone_images = train_data[idx_train]</span><br><span class="line">dslr_images = train_answ[idx_train]</span><br><span class="line">_, temp_dslr, loss_g_dslr = sess.run([train_g_dslr, fake_dslr, g_loss_dslr],</span><br><span class="line">                                     feed_dict=&#123;phone_: phone_images, dslr_: dslr_images&#125;) <span class="comment"># train_g_dslr就对应上一部分的相应的参数</span></span><br><span class="line">temp_dslr1 = fake_image_pool(number_fake_inputs, temp_dslr, fake_images_dslr)</span><br><span class="line"><span class="comment"># train discriminator A</span></span><br><span class="line">_ = sess.run([train_d_dslr], feed_dict=&#123;phone_: phone_images, dslr_: dslr_images, fake_dslr_pool: temp_dslr1&#125;) <span class="comment"># train_d_dslr就对应上一部分的相应的参数</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># train generator B</span></span><br><span class="line">_, temp_phone, loss_g_phone = sess.run([train_g_phone, fake_phone, g_loss_phone],</span><br><span class="line">                                       feed_dict=&#123;phone_: phone_images, dslr_: dslr_images&#125;) <span class="comment"># train_g_phone就对应上一部分的相应的参数</span></span><br><span class="line"><span class="comment"># train discriminator B</span></span><br><span class="line">temp_phone1 = fake_image_pool(number_fake_inputs, temp_phone, fake_images_phone)</span><br><span class="line">_ = sess.run([train_d_phone], feed_dict=&#123;phone_: phone_images, dslr_: dslr_images, fake_phone_pool: temp_phone1&#125;) <span class="comment"># train_d_phone就对应上一部分的相应的参数</span></span><br></pre></td></tr></table></figure>

</li>
</ol>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><ol>
<li>cycleGAN是用来做图像翻译的，可以作为现在无监督图像翻译方法的baseline</li>
<li>cycleGAN重点要理解其网络的意义，循环一致性损失的优越性能</li>
<li>网络部分如果有疑惑，可以在联系我。</li>
</ol>
]]></content>
      <categories>
        <category>深度学习</category>
        <category>论文理解</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title>tensorflow实现sobel梯度算法</title>
    <url>/2020/02/28/tensorflow%E5%AE%9E%E7%8E%B0sobel%E6%A2%AF%E5%BA%A6%E7%AE%97%E6%B3%95/</url>
    <content><![CDATA[<p>tensorflow中使用sobel算子对[bs , h, w, c]的图像梯度<a id="more"></a></p>
<h3 id="Sobel-算子介绍"><a href="#Sobel-算子介绍" class="headerlink" title="Sobel 算子介绍"></a>Sobel 算子介绍</h3><p>水平方向梯度算子：</p>
<p><img src="https://i.loli.net/2020/02/28/sYzToZg275qiWvN.png" alt="1.jpg"></p>
<p>竖直方向梯度算子：</p>
<p><img src="https://i.loli.net/2020/02/28/rum9Cl3RNpGkKOs.png" alt="2.jpg"></p>
<p>图像梯度计算：</p>
<p><img src="https://i.loli.net/2020/02/28/G3ZJPzpF9fgkhwx.png" alt="3.jpg"></p>
<p>如何使用算子计算图像梯度：</p>
<p><img src="https://i.loli.net/2020/02/28/atMGXZISuPkUdA6.png" alt="4.jpg"></p>
<p><img src="https://i.loli.net/2020/02/28/WtuQeJzjZgoanr2.png" alt="5.jpg"></p>
<p>其中f(a,b), 表示图像(a,b)点的灰度值</p>
<h3 id="tensorflow中使用sobel算子计算梯度"><a href="#tensorflow中使用sobel算子计算梯度" class="headerlink" title="tensorflow中使用sobel算子计算梯度"></a>tensorflow中使用sobel算子计算梯度</h3><p>需要的包：</p>
<ul>
<li>tensorflow</li>
<li>numpy</li>
</ul>
<p>tensorflow实现函数：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sobel</span><span class="params">(input)</span>:</span></span><br><span class="line">    input_gray = tf.image.rgb_to_grayscale(input)</span><br><span class="line">    bs, w, h, c = input_gray.shape</span><br><span class="line">    Gx = np.float32(np.array([[<span class="number">-1</span>, <span class="number">0</span>, <span class="number">1</span>], [<span class="number">-2</span>, <span class="number">0</span>, <span class="number">2</span>], [<span class="number">-1</span>, <span class="number">0</span>, <span class="number">1</span>]]))</span><br><span class="line">    Gy = np.float32(np.array([[<span class="number">-1</span>, <span class="number">-2</span>, <span class="number">-1</span>], [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>], [<span class="number">1</span>, <span class="number">2</span>, <span class="number">1</span>]]))</span><br><span class="line">    Gx = tf.expand_dims(Gx, axis=<span class="number">2</span>)</span><br><span class="line">    Gx = tf.expand_dims(Gx, axis=<span class="number">3</span>)</span><br><span class="line">    Gy = tf.expand_dims(Gy, axis=<span class="number">2</span>)</span><br><span class="line">    Gy = tf.expand_dims(Gy, axis=<span class="number">3</span>)</span><br><span class="line">    x = tf.nn.conv2d(input_gray, Gx, strides=[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>], padding=<span class="string">"SAME"</span>, name=<span class="string">"hor"</span>)</span><br><span class="line">    y = tf.nn.conv2d(input_gray, Gy, strides=[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>], padding=<span class="string">"SAME"</span>, name=<span class="string">"vec"</span>)</span><br><span class="line">    G = tf.sqrt(tf.square(x) + tf.square(y))</span><br><span class="line">    <span class="keyword">return</span> G</span><br></pre></td></tr></table></figure>

<p><strong>Tips：</strong></p>
<p><strong>1. 使用tf.nn.conv2d函数时，必须保证input和filter的数据类型一致。</strong></p>
<p><strong>2.卷积操作时，input数据需是[bs, h, w, c]的形式，filter需是[ks, ks, in, out]的形式</strong></p>
<p>编写中掌握的函数：</p>
<ol>
<li><p>np.repeat()</p>
<p>该函数能复制数值，举个例子：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">gx = np.array([[<span class="number">1</span>,<span class="number">0</span>,<span class="number">1</span>],</span><br><span class="line">               [<span class="number">1</span>,<span class="number">1</span>,<span class="number">0</span>],</span><br><span class="line">               [<span class="number">0</span>,<span class="number">1</span>,<span class="number">1</span>]])</span><br><span class="line"><span class="comment"># gx 的维度是（3，3）</span></span><br><span class="line">gx = np.repeat(gx, <span class="number">2</span>, axis=<span class="number">0</span>)</span><br><span class="line"><span class="comment"># 该函数能使gx 在0维度上复制两次，变成(2, 3, 3)维度的数值，且gx[0] = gx[1]</span></span><br><span class="line">print(gx.shape)</span><br><span class="line"><span class="comment"># gx 的维度是（2，3，3）</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>np.float32()</p>
<p>该函数能将括号内的数组变成float32类型</p>
</li>
<li><p>tf.squeeze()</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">squeeze(</span><br><span class="line">    input,</span><br><span class="line">    axis=<span class="literal">None</span>,</span><br><span class="line">    name=<span class="literal">None</span>,</span><br><span class="line">    squeeze_dims=<span class="literal">None</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<p>这个函数能删除axis指定的为1的维度，比如数据a是[bs, h, w, 1]，此时使用tf.squeeze(a, axis=-1)，则数据a会变成[bs, h, w]</p>
</li>
<li><p>tf.expand_dims()</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tf.expand_dims(</span><br><span class="line">    input,</span><br><span class="line">    axis=<span class="literal">None</span>,</span><br><span class="line">    name=<span class="literal">None</span>,</span><br><span class="line">    dim=<span class="literal">None</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<p>他所实现的功能是给定一个input，在axis轴处给input增加一个为1的维度。举个例子：一个[bs, h, w]的数据a，使用tf.expand_dims(a, axis=-1)，则可以使a变成[bs, h, w, 1]的数据。</p>
</li>
</ol>
]]></content>
      <categories>
        <category>算法实现</category>
        <category>tensorflow</category>
      </categories>
      <tags>
        <tag>tensorflow</tag>
      </tags>
  </entry>
  <entry>
    <title>深度学习特征可视化</title>
    <url>/2020/02/23/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%89%B9%E5%BE%81%E5%8F%AF%E8%A7%86%E5%8C%96/</url>
    <content><![CDATA[<p>训练好的模型进行特征可视化<a id="more"></a></p>
<h3 id="实验环境-python"><a href="#实验环境-python" class="headerlink" title="实验环境(python)"></a>实验环境(python)</h3><ul>
<li>tensorflow </li>
<li>numpy</li>
<li>matplotlib</li>
<li>pillow</li>
</ul>
<h3 id="总体结构"><a href="#总体结构" class="headerlink" title="总体结构"></a>总体结构</h3><ol>
<li><h4 id="参数定义"><a href="#参数定义" class="headerlink" title="参数定义"></a>参数定义</h4><p>比如tensorflow里面占位符定义等</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">batch_size = <span class="number">1</span>; w = <span class="number">100</span>; h = <span class="number">100</span>; c = <span class="number">3</span> <span class="comment"># 图像size</span></span><br><span class="line">IMAGE_SIZE = w * h * c</span><br><span class="line">x_ = tf.placeholder(tf.float32, [batch_size, IMAGE_SIZE]) <span class="comment"># 占位符定义</span></span><br><span class="line">x_image = tf.reshape(x_, [<span class="number">-1</span>, w, h, c])</span><br></pre></td></tr></table></figure>
</li>
<li><h4 id="网络结构部分"><a href="#网络结构部分" class="headerlink" title="网络结构部分"></a>网络结构部分</h4><p>注意：在进行特征可视化时，网络结构部分使用的参数必须是训练时网络中包含的参数。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">### 函数功能封装</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">conv2d</span><span class="params">(x, W, strides=[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>], padding=<span class="string">'SAME'</span>)</span>:</span> <span class="comment"># 卷积函数</span></span><br><span class="line">    <span class="keyword">return</span> tf.nn.conv2d(x, W, strides=strides, padding=padding)</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">bias_variable</span><span class="params">(shape, name)</span>:</span> <span class="comment"># 偏置参数</span></span><br><span class="line">    initial = tf.constant_initializer(<span class="number">0.01</span>)</span><br><span class="line">    <span class="keyword">return</span> tf.get_variable(name=name, shape=shape, initializer=initial)</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">weight_variable</span><span class="params">(shape, name)</span>:</span></span><br><span class="line">    initial = tf.truncated_normal_initializer(stddev=<span class="number">0.01</span>)</span><br><span class="line">    <span class="keyword">return</span> tf.get_variable(name=name, shape=shape, initializer=initial)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">### 下面就是一个简单的卷积层</span></span><br><span class="line">W1 = weight_variable([<span class="number">9</span>, <span class="number">9</span>, <span class="number">3</span>, <span class="number">64</span>], name=<span class="string">"W1"</span>)</span><br><span class="line">b1 = bias_variable([<span class="number">64</span>], name=<span class="string">"b1"</span>)</span><br><span class="line">c1 = tf.nn.relu(conv2d(x_image, W1, strides=[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>]) + b1)</span><br></pre></td></tr></table></figure>
</li>
<li><h4 id="可视化特征"><a href="#可视化特征" class="headerlink" title="可视化特征"></a>可视化特征</h4><p><img src="https://i.loli.net/2020/02/23/cfqPO1jroEwxZFh.png" alt="image-20200223211041500.png"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    saver = tf.train.Saver()</span><br><span class="line">    saver.restore(sess, <span class="string">"your model"</span>) <span class="comment"># 通过这两行能加载模型</span></span><br><span class="line">    image = Image.open(<span class="string">"pictures"</span>) <span class="comment"># 加载图片，用于喂前面的占位符，图像大小要与前面定义的参数一样</span></span><br><span class="line">    features = sess.run([enhanced, c1], feed_dict=&#123;x_: image&#125;)</span><br><span class="line">    <span class="comment">## 输出特征</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(index):</span><br><span class="line">        plt.matshow(features[<span class="number">0</span>, :, :, i])</span><br><span class="line">        plt.savefig(<span class="string">"save features"</span>)</span><br></pre></td></tr></table></figure>

<p>输入的图像：</p>
<p><img src="https://i.loli.net/2020/02/23/qdD2gF4TXHetMGc.jpg" alt="0.jpg"></p>
<p>特征可视化如图所示：</p>
<p><img src="https://i.loli.net/2020/02/23/mHTISZa5AoBELCb.png" alt="1.png"></p>
<p><img src="https://i.loli.net/2020/02/23/3yq7mdCZrBEL6aj.png" alt="4.png"></p>
<p><img src="https://i.loli.net/2020/02/23/q8weUjLKAar9MfW.png" alt="2.png"></p>
</li>
</ol>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>大致分成三个部分：1.参数定义；2.网络结构定义；3.特征可视化</p>
]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title>论文复现（一）</title>
    <url>/2020/02/16/%E8%AE%BA%E6%96%87%E5%A4%8D%E7%8E%B0%EF%BC%88%E4%B8%80%EF%BC%89/</url>
    <content><![CDATA[<h3 id="Low-Light-Image-Enhancement-via-a-Deep-Hybrid-Network"><a href="#Low-Light-Image-Enhancement-via-a-Deep-Hybrid-Network" class="headerlink" title="Low-Light Image Enhancement via a Deep Hybrid Network"></a>Low-Light Image Enhancement via a Deep Hybrid Network<a id="more"></a></h3><p>最近做图像增强方面的研究，看到了一篇2019年的TIP文章<a href="https://ieeexplore.ieee.org/document/8692732" target="_blank" rel="noopener">low-light image enhacement via a deep hybrid network</a>，为了看看真正的结果，我在DPED数据集上进行了测试。论文采用了生成对抗网络的结构，但是生成器仅仅给了网络结构图，判别器倒是给了具体的参数，因此我仅给出生成器的复现。基于的是tensorflow平台。</p>
<p><img src="https://i.loli.net/2020/02/16/a8yOWo4cbnF1d9V.png" alt="image-20200216222113402.png"></p>
<p>生成器代码如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LRNN</span><span class="params">(tf.keras.layers.Layer)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, horizontal, reverse, **kwargs)</span>:</span></span><br><span class="line">        self.horizontal = horizontal</span><br><span class="line">        self.reverse = reverse</span><br><span class="line">        super(LRNN, self).__init__(**kwargs)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">build</span><span class="params">(self, input_shape)</span>:</span></span><br><span class="line">        super(LRNN, self).build(input_shape)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">compute_output_shape</span><span class="params">(self, input_shape)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> tuple(input_shape[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">reorder_input</span><span class="params">(self, X)</span>:</span></span><br><span class="line">        <span class="comment"># X.shape = (batch_size, row, column, channel)</span></span><br><span class="line">        <span class="keyword">if</span> self.horizontal:</span><br><span class="line">            X = tf.keras.backend.permute_dimensions(X, (<span class="number">2</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">3</span>))</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            X = tf.keras.backend.permute_dimensions(X, (<span class="number">1</span>, <span class="number">0</span>, <span class="number">2</span>, <span class="number">3</span>))</span><br><span class="line">        <span class="keyword">if</span> self.reverse:</span><br><span class="line">            X = tf.keras.backend.reverse(X, <span class="number">0</span>)</span><br><span class="line">        <span class="keyword">return</span> X</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">reorder_output</span><span class="params">(self, X)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> self.reverse:</span><br><span class="line">            X = tf.keras.backend.reverse(X, <span class="number">0</span>)</span><br><span class="line">        <span class="keyword">if</span> self.horizontal:</span><br><span class="line">            X = tf.keras.backend.permute_dimensions(X, (<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>, <span class="number">3</span>))</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            X = tf.keras.backend.permute_dimensions(X, (<span class="number">1</span>, <span class="number">0</span>, <span class="number">2</span>, <span class="number">3</span>))</span><br><span class="line">        <span class="keyword">return</span> X</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">call</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        [X, P, Q] = x</span><br><span class="line">        X = self.reorder_input(X)</span><br><span class="line">        P = self.reorder_input(P)</span><br><span class="line">        Q = self.reorder_input(Q)</span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">compute</span><span class="params">(a, x)</span>:</span></span><br><span class="line">            H = a</span><br><span class="line">            X, P, Q = x</span><br><span class="line">            H = P * H + Q * X</span><br><span class="line">            <span class="keyword">return</span> H</span><br><span class="line"></span><br><span class="line">        initializer = tf.zeros_like(X[<span class="number">0</span>])</span><br><span class="line">        S = tf.scan(compute, (X, P, Q), initializer)</span><br><span class="line">        H = self.reorder_output(S)</span><br><span class="line">        <span class="keyword">return</span> H</span><br><span class="line">    </span><br><span class="line">   <span class="function"><span class="keyword">def</span> <span class="title">generator</span><span class="params">(input_image, batch_size = batch_size)</span>:</span></span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">"generator"</span>):</span><br><span class="line">        <span class="comment"># 这个地方要换成梯度图</span></span><br><span class="line">        input_image_ = input_image[:, :, :, <span class="number">0</span>] * <span class="number">0.299</span> + \</span><br><span class="line">                       input_image[:, :, :, <span class="number">1</span>] * <span class="number">0.587</span> + \</span><br><span class="line">                       input_image[:, :, :, <span class="number">2</span>] * <span class="number">0.114</span></span><br><span class="line">        w = input_image_.shape[<span class="number">1</span>]</span><br><span class="line">        h = input_image_.shape[<span class="number">2</span>]</span><br><span class="line">        gradients = tf.reshape(input_image_, shape=[<span class="number">-1</span>, w, h, <span class="number">1</span>])</span><br><span class="line">        <span class="comment"># edge stream</span></span><br><span class="line">        <span class="comment"># stage 1</span></span><br><span class="line">        w_1_1 = weight_variable([<span class="number">3</span>, <span class="number">3</span>, <span class="number">3</span>, <span class="number">32</span>], name=<span class="string">"w_1_1"</span>)</span><br><span class="line">        conv_1_1 = tf.nn.relu(conv2d(input_image, w_1_1, strides=[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>]))</span><br><span class="line">        w_1_2 = weight_variable([<span class="number">3</span>, <span class="number">3</span>, <span class="number">32</span>, <span class="number">32</span>], name=<span class="string">"w_1_2"</span>)</span><br><span class="line">        conv_1_2 = tf.nn.relu(conv2d(conv_1_1, w_1_2, strides=[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>]))</span><br><span class="line">        w_1_3 = weight_variable([<span class="number">3</span>, <span class="number">3</span>, <span class="number">32</span>, <span class="number">32</span>], name=<span class="string">"w_1_3"</span>)</span><br><span class="line">        conv_1_3 = tf.nn.relu(conv2d(conv_1_2, w_1_3, strides=[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>]))</span><br><span class="line">        w_1_4 = weight_variable([<span class="number">3</span>, <span class="number">3</span>, <span class="number">32</span>, <span class="number">32</span>], name=<span class="string">"w_1_4"</span>)</span><br><span class="line">        conv_1_4 = tf.nn.relu(conv2d(conv_1_3, w_1_4, strides=[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>]))</span><br><span class="line">        w_1_5 = weight_variable([<span class="number">3</span>, <span class="number">3</span>, <span class="number">32</span>, <span class="number">32</span>], name=<span class="string">"w_1_5"</span>)</span><br><span class="line">        conv_1_5 = tf.nn.relu(conv2d(conv_1_4, w_1_5, strides=[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>]))</span><br><span class="line">        w_1_6 = weight_variable([<span class="number">3</span>, <span class="number">3</span>, <span class="number">32</span>, <span class="number">32</span>], name=<span class="string">"w_1_6"</span>)</span><br><span class="line">        conv_1_6 = tf.nn.relu(conv2d(conv_1_5, w_1_6, strides=[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>]))</span><br><span class="line">        w_1_7 = weight_variable([<span class="number">3</span>, <span class="number">3</span>, <span class="number">32</span>, <span class="number">64</span>], name=<span class="string">"w_1_7"</span>)</span><br><span class="line">        conv_1_7 = tf.nn.relu(conv2d(conv_1_6, w_1_7, strides=[<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>]))</span><br><span class="line">        <span class="comment"># stage 2</span></span><br><span class="line">        w_2_1 = weight_variable([<span class="number">3</span>, <span class="number">3</span>, <span class="number">64</span>, <span class="number">64</span>], name=<span class="string">"w_2_1"</span>)</span><br><span class="line">        conv_2_1 = tf.nn.relu(conv2d(conv_1_7, w_2_1, strides=[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>]))</span><br><span class="line">        w_2_2 = weight_variable([<span class="number">3</span>, <span class="number">3</span>, <span class="number">64</span>, <span class="number">64</span>], name=<span class="string">"w_2_2"</span>)</span><br><span class="line">        conv_2_2 = tf.nn.relu(conv2d(conv_2_1, w_2_2, strides=[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>]))</span><br><span class="line">        w_2_3 = weight_variable([<span class="number">3</span>, <span class="number">3</span>, <span class="number">64</span>, <span class="number">64</span>], name=<span class="string">"w_2_3"</span>)</span><br><span class="line">        conv_2_3 = tf.nn.relu(conv2d(conv_2_2, w_2_3, strides=[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>]))</span><br><span class="line">        w_2_4 = weight_variable([<span class="number">3</span>, <span class="number">3</span>, <span class="number">64</span>, <span class="number">64</span>], name=<span class="string">"w_2_4"</span>)</span><br><span class="line">        conv_2_4 = tf.nn.relu(conv2d(conv_2_3, w_2_4, strides=[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>]))</span><br><span class="line">        w_2_5 = weight_variable([<span class="number">3</span>, <span class="number">3</span>, <span class="number">64</span>, <span class="number">64</span>], name=<span class="string">"w_2_5"</span>)</span><br><span class="line">        conv_2_5 = tf.nn.relu(conv2d(conv_2_4, w_2_5, strides=[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>]))</span><br><span class="line">        w_2_6 = weight_variable([<span class="number">3</span>, <span class="number">3</span>, <span class="number">64</span>, <span class="number">64</span>], name=<span class="string">"w_2_6"</span>)</span><br><span class="line">        conv_2_6 = tf.nn.relu(conv2d(conv_2_5, w_2_6, strides=[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>]))</span><br><span class="line">        w_2_7 = weight_variable([<span class="number">3</span>, <span class="number">3</span>, <span class="number">64</span>, <span class="number">128</span>], name=<span class="string">"w_2_7"</span>)</span><br><span class="line">        conv_2_7 = tf.nn.relu(conv2d(conv_2_6, w_2_7, strides=[<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>]))</span><br><span class="line">        <span class="comment"># stage 3</span></span><br><span class="line">        w_3_1 = weight_variable([<span class="number">3</span>, <span class="number">3</span>, <span class="number">128</span>, <span class="number">128</span>], name=<span class="string">"w_3_1"</span>)</span><br><span class="line">        conv_3_1 = tf.nn.relu(conv2d(conv_2_7, w_3_1, strides=[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>]))</span><br><span class="line">        w_3_2 = weight_variable([<span class="number">3</span>, <span class="number">3</span>, <span class="number">128</span>, <span class="number">128</span>], name=<span class="string">"w_3_2"</span>)</span><br><span class="line">        conv_3_2 = tf.nn.relu(conv2d(conv_3_1, w_3_2, strides=[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>]))</span><br><span class="line">        w_3_3 = weight_variable([<span class="number">3</span>, <span class="number">3</span>, <span class="number">128</span>, <span class="number">128</span>], name=<span class="string">"w_3_3"</span>)</span><br><span class="line">        conv_3_3 = tf.nn.relu(conv2d(conv_3_2, w_3_3, strides=[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>]))</span><br><span class="line">        w_3_4 = weight_variable([<span class="number">3</span>, <span class="number">3</span>, <span class="number">128</span>, <span class="number">128</span>], name=<span class="string">"w_3_4"</span>)</span><br><span class="line">        conv_3_4 = tf.nn.relu(conv2d(conv_3_3, w_3_4, strides=[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>]))</span><br><span class="line">        w_3_5 = weight_variable([<span class="number">3</span>, <span class="number">3</span>, <span class="number">128</span>, <span class="number">128</span>], name=<span class="string">"w_3_5"</span>)</span><br><span class="line">        conv_3_5 = tf.nn.relu(conv2d(conv_3_4, w_3_5, strides=[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>]))</span><br><span class="line">        w_3_6 = weight_variable([<span class="number">3</span>, <span class="number">3</span>, <span class="number">128</span>, <span class="number">128</span>], name=<span class="string">"w_3_6"</span>)</span><br><span class="line">        conv_3_6 = tf.nn.relu(conv2d(conv_3_5, w_3_6, strides=[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>]))</span><br><span class="line">        w_3_7 = weight_variable([<span class="number">3</span>, <span class="number">3</span>, <span class="number">128</span>, <span class="number">128</span>], name=<span class="string">"w_3_7"</span>)</span><br><span class="line">        conv_3_7 = tf.nn.relu(conv2d(conv_3_6, w_3_7, strides=[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>]))</span><br><span class="line">        w_3_8 = weight_variable([<span class="number">3</span>, <span class="number">3</span>, <span class="number">128</span>, <span class="number">128</span>], name=<span class="string">"w_3_8"</span>)</span><br><span class="line">        conv_3_8 = tf.nn.relu(conv2d(conv_3_7, w_3_8, strides=[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>]))</span><br><span class="line">        w_3_9 = weight_variable([<span class="number">3</span>, <span class="number">3</span>, <span class="number">128</span>, <span class="number">128</span>], name=<span class="string">"w_3_9"</span>)</span><br><span class="line">        conv_3_9 = tf.nn.relu(conv2d(conv_3_8, w_3_9, strides=[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>]))</span><br><span class="line">        <span class="comment"># stage 4</span></span><br><span class="line">        w_4_1 = weight_variable([<span class="number">3</span>, <span class="number">3</span>, <span class="number">128</span>, <span class="number">128</span>], name=<span class="string">"w_4_1"</span>)</span><br><span class="line">        conv_4_1 = tf.nn.relu(conv2d(conv_3_9, w_4_1, strides=[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>]))</span><br><span class="line">        w_4_2 = weight_variable([<span class="number">3</span>, <span class="number">3</span>, <span class="number">128</span>, <span class="number">128</span>], name=<span class="string">"w_4_2"</span>)</span><br><span class="line">        conv_4_2 = tf.nn.relu(conv2d(conv_4_1, w_4_2, strides=[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>]))</span><br><span class="line">        w_4_3 = weight_variable([<span class="number">3</span>, <span class="number">3</span>, <span class="number">128</span>, <span class="number">128</span>], name=<span class="string">"w_4_3"</span>)</span><br><span class="line">        conv_4_3 = tf.nn.relu(conv2d(conv_4_2, w_4_3, strides=[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>]))</span><br><span class="line">        w_4_4 = weight_variable([<span class="number">3</span>, <span class="number">3</span>, <span class="number">128</span>, <span class="number">128</span>], name=<span class="string">"w_4_4"</span>)</span><br><span class="line">        conv_4_4 = tf.nn.relu(conv2d(conv_4_3, w_4_4, strides=[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>]))</span><br><span class="line">        w_4_5 = weight_variable([<span class="number">3</span>, <span class="number">3</span>, <span class="number">128</span>, <span class="number">128</span>], name=<span class="string">"w_4_5"</span>)</span><br><span class="line">        conv_4_5 = tf.nn.relu(conv2d(conv_4_4, w_4_5, strides=[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>]))</span><br><span class="line">        w_4_6 = weight_variable([<span class="number">3</span>, <span class="number">3</span>, <span class="number">128</span>, <span class="number">128</span>], name=<span class="string">"w_4_6"</span>)</span><br><span class="line">        conv_4_6 = tf.nn.relu(conv2d(conv_4_5, w_4_6, strides=[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>]))</span><br><span class="line">        w_4_7 = weight_variable([<span class="number">3</span>, <span class="number">3</span>, <span class="number">128</span>, <span class="number">128</span>], name=<span class="string">"w_4_7"</span>)</span><br><span class="line">        conv_4_7 = tf.nn.relu(conv2d(conv_4_6, w_4_7, strides=[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>]))</span><br><span class="line">        w_4_8 = weight_variable([<span class="number">3</span>, <span class="number">3</span>, <span class="number">128</span>, <span class="number">128</span>], name=<span class="string">"w_4_8"</span>)</span><br><span class="line">        conv_4_8 = tf.nn.relu(conv2d(conv_4_7, w_4_8, strides=[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>]))</span><br><span class="line">        w_4_9 = weight_variable([<span class="number">3</span>, <span class="number">3</span>, <span class="number">128</span>, <span class="number">128</span>], name=<span class="string">"w_4_9"</span>)</span><br><span class="line">        conv_4_9 = tf.nn.relu(conv2d(conv_4_8, w_4_9, strides=[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>]))</span><br><span class="line">        conv_4_9 = conv_4_9 + conv_3_9</span><br><span class="line">        <span class="comment"># stage 5</span></span><br><span class="line">        conv_5_1 = tf.nn.relu(deconv2d(conv_4_9, <span class="number">64</span>, <span class="number">3</span>))</span><br><span class="line">        w_5_2 = weight_variable([<span class="number">3</span>, <span class="number">3</span>, <span class="number">64</span>, <span class="number">64</span>], name=<span class="string">"w_5_1"</span>)</span><br><span class="line">        conv_5_2 = tf.nn.relu(conv2d(conv_5_1, w_5_2, strides=[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>]))</span><br><span class="line">        w_5_3 = weight_variable([<span class="number">3</span>, <span class="number">3</span>, <span class="number">64</span>, <span class="number">64</span>], name=<span class="string">"w_5_3"</span>)</span><br><span class="line">        conv_5_3 = tf.nn.relu(conv2d(conv_5_2, w_5_3, strides=[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>]))</span><br><span class="line">        w_5_4 = weight_variable([<span class="number">3</span>, <span class="number">3</span>, <span class="number">64</span>, <span class="number">64</span>], name=<span class="string">"w_5_4"</span>)</span><br><span class="line">        conv_5_4 = tf.nn.relu(conv2d(conv_5_3, w_5_4, strides=[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>]))</span><br><span class="line">        w_5_5 = weight_variable([<span class="number">3</span>, <span class="number">3</span>, <span class="number">64</span>, <span class="number">64</span>], name=<span class="string">"w_5_5"</span>)</span><br><span class="line">        conv_5_5 = tf.nn.relu(conv2d(conv_5_4, w_5_5, strides=[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>]))</span><br><span class="line">        w_5_6 = weight_variable([<span class="number">3</span>, <span class="number">3</span>, <span class="number">64</span>, <span class="number">64</span>], name=<span class="string">"w_5_6"</span>)</span><br><span class="line">        conv_5_6 = tf.nn.relu(conv2d(conv_5_5, w_5_6, strides=[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>]))</span><br><span class="line">        w_5_7 = weight_variable([<span class="number">3</span>, <span class="number">3</span>, <span class="number">64</span>, <span class="number">64</span>], name=<span class="string">"w_5_7"</span>)</span><br><span class="line">        conv_5_7 = tf.nn.relu(conv2d(conv_5_6, w_5_7, strides=[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>]))</span><br><span class="line">        conv_5_7 = conv_5_7 + conv_2_6</span><br><span class="line">        <span class="comment"># stage 6</span></span><br><span class="line">        conv_6_1 = tf.nn.relu(deconv2d(conv_5_7, <span class="number">64</span>, <span class="number">3</span>))</span><br><span class="line">        w_6_2 = weight_variable([<span class="number">3</span>, <span class="number">3</span>, <span class="number">64</span>, <span class="number">128</span>], name=<span class="string">"w_6_2"</span>)</span><br><span class="line">        conv_6_2 = tf.nn.relu(conv2d(conv_6_1, w_6_2, strides=[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>]))</span><br><span class="line">        w_6_3 = weight_variable([<span class="number">3</span>, <span class="number">3</span>, <span class="number">128</span>, <span class="number">128</span>], name=<span class="string">"w_6_3"</span>)</span><br><span class="line">        conv_6_3 = tf.nn.relu(conv2d(conv_6_2, w_6_3, strides=[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>]))</span><br><span class="line">        w_6_4 = weight_variable([<span class="number">3</span>, <span class="number">3</span>, <span class="number">128</span>, <span class="number">128</span>], name=<span class="string">"w_6_4"</span>)</span><br><span class="line">        conv_6_4 = tf.nn.relu(conv2d(conv_6_3, w_6_4, strides=[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>]))</span><br><span class="line">        w_6_5 = weight_variable([<span class="number">3</span>, <span class="number">3</span>, <span class="number">128</span>, <span class="number">128</span>], name=<span class="string">"w_6_5"</span>)</span><br><span class="line">        conv_6_5 = tf.nn.relu(conv2d(conv_6_4, w_6_5, strides=[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>]))</span><br><span class="line">        w_6_6 = weight_variable([<span class="number">3</span>, <span class="number">3</span>, <span class="number">128</span>, <span class="number">128</span>], name=<span class="string">"w_6_6"</span>)</span><br><span class="line">        conv_6_6 = tf.nn.relu(conv2d(conv_6_5, w_6_6, strides=[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>]))</span><br><span class="line">        w_6_7 = weight_variable([<span class="number">3</span>, <span class="number">3</span>, <span class="number">128</span>, <span class="number">128</span>], name=<span class="string">"w_6_7"</span>)</span><br><span class="line">        conv_6_7 = tf.nn.relu(conv2d(conv_6_6, w_6_7, strides=[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>]))</span><br><span class="line">        w_p1 = conv_6_7[:, :, :, <span class="number">0</span>:<span class="number">16</span>]</span><br><span class="line">        w_p2 = conv_6_7[:, :, :, <span class="number">16</span>:<span class="number">32</span>]</span><br><span class="line">        w_p3 = conv_6_7[:, :, :, <span class="number">32</span>:<span class="number">48</span>]</span><br><span class="line">        w_p4 = conv_6_7[:, :, :, <span class="number">48</span>:<span class="number">64</span>]</span><br><span class="line">        w_q1 = conv_6_7[:, :, :, <span class="number">64</span>:<span class="number">80</span>]</span><br><span class="line">        w_q2 = conv_6_7[:, :, :, <span class="number">80</span>:<span class="number">96</span>]</span><br><span class="line">        w_q3 = conv_6_7[:, :, :, <span class="number">96</span>:<span class="number">112</span>]</span><br><span class="line">        w_q4 = conv_6_7[:, :, :, <span class="number">112</span>:<span class="number">128</span>]</span><br><span class="line">        <span class="comment"># out = [batch, w, h, 128] --&gt; </span></span><br><span class="line">        <span class="comment">#   2 * [batch, w, h, 64] --&gt; </span></span><br><span class="line">        <span class="comment">#   2 * four directions [batch, w, h, 16]</span></span><br><span class="line">        <span class="comment"># content stream</span></span><br><span class="line">        <span class="comment"># stage 1</span></span><br><span class="line"></span><br><span class="line">        w_c_1 = weight_variable([<span class="number">3</span>, <span class="number">3</span>, <span class="number">3</span>, <span class="number">32</span>], name=<span class="string">"w_c_1"</span>)</span><br><span class="line">        conv_c_1 = tf.nn.relu(tf.nn.atrous_conv2d(input_image, filters=w_c_1, rate=<span class="number">2</span>, padding=<span class="string">'SAME'</span>))</span><br><span class="line">        w_c_2 = weight_variable([<span class="number">3</span>, <span class="number">3</span>, <span class="number">32</span>, <span class="number">32</span>], name=<span class="string">"w_2"</span>)</span><br><span class="line">        conv_c_2 = tf.nn.relu(tf.nn.atrous_conv2d(conv_c_1, filters=w_c_2, rate=<span class="number">2</span>, padding=<span class="string">'SAME'</span>))</span><br><span class="line">        w_c_1_7 = weight_variable([<span class="number">3</span>, <span class="number">3</span>, <span class="number">32</span>, <span class="number">64</span>], name=<span class="string">"w_c_1_7"</span>)</span><br><span class="line">        conv_c_1_7 = tf.nn.relu(conv2d(conv_c_2, w_c_1_7, strides=[<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>]))</span><br><span class="line">        <span class="comment"># stage 2</span></span><br><span class="line">        w_c_2_1 = weight_variable([<span class="number">3</span>, <span class="number">3</span>, <span class="number">64</span>, <span class="number">64</span>], name=<span class="string">"w_c_2_1"</span>)</span><br><span class="line">        conv_c_2_1 = tf.nn.relu(conv2d(conv_c_1_7, w_c_2_1, strides=[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>]))</span><br><span class="line">        w_c_2_2 = weight_variable([<span class="number">3</span>, <span class="number">3</span>, <span class="number">64</span>, <span class="number">64</span>], name=<span class="string">"w_c_2_2"</span>)</span><br><span class="line">        conv_c_2_2 = tf.nn.relu(conv2d(conv_c_2_1, w_c_2_2, strides=[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>]))</span><br><span class="line">        w_c_2_3 = weight_variable([<span class="number">3</span>, <span class="number">3</span>, <span class="number">64</span>, <span class="number">64</span>], name=<span class="string">"w_c_2_3"</span>)</span><br><span class="line">        conv_c_2_3 = tf.nn.relu(conv2d(conv_c_2_2, w_c_2_3, strides=[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>]))</span><br><span class="line">        w_c_2_4 = weight_variable([<span class="number">3</span>, <span class="number">3</span>, <span class="number">64</span>, <span class="number">64</span>], name=<span class="string">"w_c_2_4"</span>)</span><br><span class="line">        conv_c_2_4 = tf.nn.relu(conv2d(conv_c_2_3, w_c_2_4, strides=[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>]))</span><br><span class="line">        w_c_2_5 = weight_variable([<span class="number">3</span>, <span class="number">3</span>, <span class="number">64</span>, <span class="number">64</span>], name=<span class="string">"w_c_2_5"</span>)</span><br><span class="line">        conv_c_2_5 = tf.nn.relu(conv2d(conv_c_2_4, w_c_2_5, strides=[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>]))</span><br><span class="line">        w_c_2_6 = weight_variable([<span class="number">3</span>, <span class="number">3</span>, <span class="number">64</span>, <span class="number">64</span>], name=<span class="string">"w_c_2_6"</span>)</span><br><span class="line">        conv_c_2_6 = tf.nn.relu(conv2d(conv_c_2_5, w_c_2_6, strides=[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>]))</span><br><span class="line">        w_c_2_7 = weight_variable([<span class="number">3</span>, <span class="number">3</span>, <span class="number">64</span>, <span class="number">128</span>], name=<span class="string">"w_c_2_7"</span>)</span><br><span class="line">        conv_c_2_7 = tf.nn.relu(conv2d(conv_c_2_6, w_c_2_7, strides=[<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>]))</span><br><span class="line">        <span class="comment"># stage 3</span></span><br><span class="line">        w_c_3_1 = weight_variable([<span class="number">3</span>, <span class="number">3</span>, <span class="number">128</span>, <span class="number">128</span>], name=<span class="string">"w_c_3_1"</span>)</span><br><span class="line">        conv_c_3_1 = tf.nn.relu(conv2d(conv_c_2_7, w_c_3_1, strides=[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>]))</span><br><span class="line">        w_c_3_2 = weight_variable([<span class="number">3</span>, <span class="number">3</span>, <span class="number">128</span>, <span class="number">128</span>], name=<span class="string">"w_c_3_2"</span>)</span><br><span class="line">        conv_c_3_2 = tf.nn.relu(conv2d(conv_c_3_1, w_c_3_2, strides=[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>]))</span><br><span class="line">        w_c_3_3 = weight_variable([<span class="number">3</span>, <span class="number">3</span>, <span class="number">128</span>, <span class="number">128</span>], name=<span class="string">"w_c_3_3"</span>)</span><br><span class="line">        conv_c_3_3 = tf.nn.relu(conv2d(conv_c_3_2, w_c_3_3, strides=[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>]))</span><br><span class="line">        w_c_3_4 = weight_variable([<span class="number">3</span>, <span class="number">3</span>, <span class="number">128</span>, <span class="number">128</span>], name=<span class="string">"w_c_3_4"</span>)</span><br><span class="line">        conv_c_3_4 = tf.nn.relu(conv2d(conv_c_3_3, w_c_3_4, strides=[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>]))</span><br><span class="line">        w_c_3_5 = weight_variable([<span class="number">3</span>, <span class="number">3</span>, <span class="number">128</span>, <span class="number">128</span>], name=<span class="string">"w_c_3_5"</span>)</span><br><span class="line">        conv_c_3_5 = tf.nn.relu(conv2d(conv_c_3_4, w_c_3_5, strides=[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>]))</span><br><span class="line">        w_c_3_6 = weight_variable([<span class="number">3</span>, <span class="number">3</span>, <span class="number">128</span>, <span class="number">128</span>], name=<span class="string">"w_c_3_6"</span>)</span><br><span class="line">        conv_c_3_6 = tf.nn.relu(conv2d(conv_c_3_5, w_c_3_6, strides=[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>]))</span><br><span class="line">        w_c_3_7 = weight_variable([<span class="number">3</span>, <span class="number">3</span>, <span class="number">128</span>, <span class="number">128</span>], name=<span class="string">"w_c_3_7"</span>)</span><br><span class="line">        conv_c_3_7 = tf.nn.relu(conv2d(conv_c_3_6, w_c_3_7, strides=[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>]))</span><br><span class="line">        w_c_3_8 = weight_variable([<span class="number">3</span>, <span class="number">3</span>, <span class="number">128</span>, <span class="number">128</span>], name=<span class="string">"w_c_3_8"</span>)</span><br><span class="line">        conv_c_3_8 = tf.nn.relu(conv2d(conv_c_3_7, w_c_3_8, strides=[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>]))</span><br><span class="line">        w_c_3_9 = weight_variable([<span class="number">3</span>, <span class="number">3</span>, <span class="number">128</span>, <span class="number">128</span>], name=<span class="string">"w_c_3_9"</span>)</span><br><span class="line">        conv_c_3_9 = tf.nn.relu(conv2d(conv_c_3_8, w_c_3_9, strides=[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>]))</span><br><span class="line">        <span class="comment"># stage 4</span></span><br><span class="line">        w_c_4_1 = weight_variable([<span class="number">3</span>, <span class="number">3</span>, <span class="number">128</span>, <span class="number">128</span>], name=<span class="string">"w_4_c_1"</span>)</span><br><span class="line">        conv_c_4_1 = tf.nn.relu(conv2d(conv_c_3_9, w_c_4_1, strides=[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>]))</span><br><span class="line">        w_c_4_2 = weight_variable([<span class="number">3</span>, <span class="number">3</span>, <span class="number">128</span>, <span class="number">128</span>], name=<span class="string">"w_4_c_2"</span>)</span><br><span class="line">        conv_c_4_2 = tf.nn.relu(conv2d(conv_c_4_1, w_c_4_2, strides=[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>]))</span><br><span class="line">        w_c_4_3 = weight_variable([<span class="number">3</span>, <span class="number">3</span>, <span class="number">128</span>, <span class="number">128</span>], name=<span class="string">"w_4_c_3"</span>)</span><br><span class="line">        conv_c_4_3 = tf.nn.relu(conv2d(conv_c_4_2, w_c_4_3, strides=[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>]))</span><br><span class="line">        conv_c_4_3 = conv_c_4_3 + conv_4_3</span><br><span class="line">        w_c_4_4 = weight_variable([<span class="number">3</span>, <span class="number">3</span>, <span class="number">128</span>, <span class="number">128</span>], name=<span class="string">"w_4_c_4"</span>)</span><br><span class="line">        conv_c_4_4 = tf.nn.relu(conv2d(conv_c_4_3, w_c_4_4, strides=[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>]))</span><br><span class="line">        w_c_4_5 = weight_variable([<span class="number">3</span>, <span class="number">3</span>, <span class="number">128</span>, <span class="number">128</span>], name=<span class="string">"w_4_c_5"</span>)</span><br><span class="line">        conv_c_4_5 = tf.nn.relu(conv2d(conv_c_4_4, w_c_4_5, strides=[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>]))</span><br><span class="line">        w_c_4_6 = weight_variable([<span class="number">3</span>, <span class="number">3</span>, <span class="number">128</span>, <span class="number">128</span>], name=<span class="string">"w_4_c_6"</span>)</span><br><span class="line">        conv_c_4_6 = tf.nn.relu(conv2d(conv_c_4_5, w_c_4_6, strides=[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>]))</span><br><span class="line">        w_c_4_7 = weight_variable([<span class="number">3</span>, <span class="number">3</span>, <span class="number">128</span>, <span class="number">128</span>], name=<span class="string">"w_4_c_7"</span>)</span><br><span class="line">        conv_c_4_7 = tf.nn.relu(conv2d(conv_c_4_6, w_c_4_7, strides=[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>]))</span><br><span class="line">        w_c_4_8 = weight_variable([<span class="number">3</span>, <span class="number">3</span>, <span class="number">128</span>, <span class="number">128</span>], name=<span class="string">"w_4_c_8"</span>)</span><br><span class="line">        conv_c_4_8 = tf.nn.relu(conv2d(conv_c_4_7, w_c_4_8, strides=[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>]))</span><br><span class="line">        w_c_4_9 = weight_variable([<span class="number">3</span>, <span class="number">3</span>, <span class="number">128</span>, <span class="number">128</span>], name=<span class="string">"w_4_c_9"</span>)</span><br><span class="line">        conv_c_4_9 = tf.nn.relu(conv2d(conv_c_4_8, w_c_4_9, strides=[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>]))</span><br><span class="line">        conv_c_4_9 = conv_c_4_9 + conv_c_3_9</span><br><span class="line">        <span class="comment"># stage 5</span></span><br><span class="line">        conv_c_5_1 = tf.nn.relu(deconv2d(conv_c_4_9, <span class="number">64</span>, <span class="number">3</span>))</span><br><span class="line">        conv_c_5_1 = conv_c_5_1 + conv_5_1</span><br><span class="line">        w_c_5_2 = weight_variable([<span class="number">3</span>, <span class="number">3</span>, <span class="number">64</span>, <span class="number">64</span>], name=<span class="string">"w_c_5_1"</span>)</span><br><span class="line">        conv_c_5_2 = tf.nn.relu(conv2d(conv_c_5_1, w_c_5_2, strides=[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>]))</span><br><span class="line">        w_c_5_3 = weight_variable([<span class="number">3</span>, <span class="number">3</span>, <span class="number">64</span>, <span class="number">64</span>], name=<span class="string">"w_c_5_3"</span>)</span><br><span class="line">        conv_c_5_3 = tf.nn.relu(conv2d(conv_c_5_2, w_c_5_3, strides=[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>]))</span><br><span class="line">        w_c_5_4 = weight_variable([<span class="number">3</span>, <span class="number">3</span>, <span class="number">64</span>, <span class="number">64</span>], name=<span class="string">"w_c_5_4"</span>)</span><br><span class="line">        conv_c_5_4 = tf.nn.relu(conv2d(conv_c_5_3, w_c_5_4, strides=[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>]))</span><br><span class="line">        w_c_5_5 = weight_variable([<span class="number">3</span>, <span class="number">3</span>, <span class="number">64</span>, <span class="number">64</span>], name=<span class="string">"w_c_5_5"</span>)</span><br><span class="line">        conv_c_5_5 = tf.nn.relu(conv2d(conv_c_5_4, w_c_5_5, strides=[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>]))</span><br><span class="line">        w_c_5_6 = weight_variable([<span class="number">3</span>, <span class="number">3</span>, <span class="number">64</span>, <span class="number">64</span>], name=<span class="string">"w_c_5_6"</span>)</span><br><span class="line">        conv_c_5_6 = tf.nn.relu(conv2d(conv_c_5_5, w_c_5_6, strides=[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>]))</span><br><span class="line">        w_c_5_7 = weight_variable([<span class="number">3</span>, <span class="number">3</span>, <span class="number">64</span>, <span class="number">64</span>], name=<span class="string">"w_c_5_7"</span>)</span><br><span class="line">        conv_c_5_7 = tf.nn.relu(conv2d(conv_c_5_6, w_c_5_7, strides=[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>]))</span><br><span class="line">        conv_c_5_7 = conv_c_5_7 + conv_c_2_6</span><br><span class="line">        <span class="comment"># stage 6</span></span><br><span class="line">        conv_c_6_1 = tf.nn.relu(deconv2d(conv_c_5_7, <span class="number">64</span>, <span class="number">3</span>))</span><br><span class="line">        conv_c_6_1 = conv_c_6_1 + conv_6_1</span><br><span class="line">        w_c_6_2 = weight_variable([<span class="number">3</span>, <span class="number">3</span>, <span class="number">64</span>, <span class="number">128</span>], name=<span class="string">"w_c_6_2"</span>)</span><br><span class="line">        conv_c_6_2 = tf.nn.relu(conv2d(conv_c_6_1, w_c_6_2, strides=[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>]))</span><br><span class="line">        w_c_6_3 = weight_variable([<span class="number">3</span>, <span class="number">3</span>, <span class="number">128</span>, <span class="number">128</span>], name=<span class="string">"w_c_6_3"</span>)</span><br><span class="line">        conv_c_6_3 = tf.nn.relu(conv2d(conv_c_6_2, w_c_6_3, strides=[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>]))</span><br><span class="line">        w_c_6_4 = weight_variable([<span class="number">3</span>, <span class="number">3</span>, <span class="number">128</span>, <span class="number">128</span>], name=<span class="string">"w_c_6_4"</span>)</span><br><span class="line">        conv_c_6_4 = tf.nn.relu(conv2d(conv_c_6_3, w_c_6_4, strides=[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>]))</span><br><span class="line">        w_c_6_5 = weight_variable([<span class="number">3</span>, <span class="number">3</span>, <span class="number">128</span>, <span class="number">128</span>], name=<span class="string">"w_c_6_5"</span>)</span><br><span class="line">        conv_c_6_5 = tf.nn.relu(conv2d(conv_c_6_4, w_c_6_5, strides=[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>]))</span><br><span class="line">        w_c_6_6 = weight_variable([<span class="number">3</span>, <span class="number">3</span>, <span class="number">128</span>, <span class="number">128</span>], name=<span class="string">"w_c_6_6"</span>)</span><br><span class="line">        conv_c_6_6 = tf.nn.relu(conv2d(conv_c_6_5, w_c_6_6, strides=[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>]))</span><br><span class="line">        w_c_6_7 = weight_variable([<span class="number">3</span>, <span class="number">3</span>, <span class="number">128</span>, <span class="number">128</span>], name=<span class="string">"w_c_6_7"</span>)</span><br><span class="line">        conv_c_6_7 = tf.nn.relu(conv2d(conv_c_6_6, w_c_6_7, strides=[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>]))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># edge stream resize flow</span></span><br><span class="line">        w_r_1 = weight_variable([<span class="number">3</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">4</span>], name=<span class="string">"w_r_1"</span>)</span><br><span class="line">        conv_r_1 = tf.nn.relu(conv2d(gradients, w_r_1, strides=[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>]))</span><br><span class="line">        w_r_2 = weight_variable([<span class="number">3</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">4</span>], name=<span class="string">"w_r_2"</span>)</span><br><span class="line">        conv_r_2 = tf.nn.relu(conv2d(conv_r_1, w_r_2, strides=[<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>]))</span><br><span class="line">        w_r_3 = weight_variable([<span class="number">3</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">4</span>], name=<span class="string">"w_r_3"</span>)</span><br><span class="line">        conv_r_3 = tf.nn.relu(conv2d(conv_r_2, w_r_3, strides=[<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>]))</span><br><span class="line">        w_r_4 = weight_variable([<span class="number">3</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">4</span>], name=<span class="string">"w_r_4"</span>)</span><br><span class="line">        conv_r_4 = tf.nn.relu(conv2d(conv_r_3, w_r_4, strides=[<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>]))</span><br><span class="line">        resize_2 = tf.image.resize_images(conv_r_2, (w, h))</span><br><span class="line">        resize_3 = tf.image.resize_images(conv_r_3, (w, h))</span><br><span class="line">        resize_4 = tf.image.resize_images(conv_r_4, (w, h))</span><br><span class="line">        concat_features = tf.concat([conv_r_1, resize_2, resize_3, resize_4], axis=<span class="number">-1</span>)</span><br><span class="line">        y1 = LRNN(horizontal=<span class="literal">True</span>, reverse=<span class="literal">False</span>)([concat_features, w_p1, w_q1])</span><br><span class="line">        y2 = LRNN(horizontal=<span class="literal">True</span>, reverse=<span class="literal">False</span>)([concat_features, w_p2, w_q2])</span><br><span class="line">        y3 = LRNN(horizontal=<span class="literal">True</span>, reverse=<span class="literal">True</span>)([concat_features, w_p1, w_q1])</span><br><span class="line">        y4 = LRNN(horizontal=<span class="literal">True</span>, reverse=<span class="literal">True</span>)([concat_features, w_p2, w_q2])</span><br><span class="line">        y5 = LRNN(horizontal=<span class="literal">False</span>, reverse=<span class="literal">False</span>)([concat_features, w_p3, w_q3])</span><br><span class="line">        y6 = LRNN(horizontal=<span class="literal">False</span>, reverse=<span class="literal">False</span>)([concat_features, w_p4, w_q4])</span><br><span class="line">        y7 = LRNN(horizontal=<span class="literal">False</span>, reverse=<span class="literal">True</span>)([concat_features, w_p3, w_q3])</span><br><span class="line">        y8 = LRNN(horizontal=<span class="literal">False</span>, reverse=<span class="literal">True</span>)([concat_features, w_p4, w_q4])</span><br><span class="line">        y12 = tf.add(y1, y2)</span><br><span class="line">        y34 = tf.add(y3, y4)</span><br><span class="line">        y56 = tf.add(y5, y6)</span><br><span class="line">        y78 = tf.add(y7, y8)</span><br><span class="line">        y_max1 = tf.maximum(y12, y34)</span><br><span class="line">        y_max2 = tf.maximum(y56, y78)</span><br><span class="line">        y = tf.maximum(y_max1, y_max2)</span><br><span class="line">        fuse_features = tf.concat([conv_c_6_7, y], axis=<span class="number">-1</span>)</span><br><span class="line">        w_f_6_2 = weight_variable([<span class="number">3</span>, <span class="number">3</span>, <span class="number">144</span>, <span class="number">128</span>], name=<span class="string">"w_f_6_2"</span>)</span><br><span class="line">        conv_f_6_2 = tf.nn.relu(conv2d(fuse_features, w_f_6_2, strides=[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>]))</span><br><span class="line">        w_f_6_3 = weight_variable([<span class="number">3</span>, <span class="number">3</span>, <span class="number">128</span>, <span class="number">128</span>], name=<span class="string">"w_f_6_3"</span>)</span><br><span class="line">        conv_f_6_3 = tf.nn.relu(conv2d(conv_f_6_2, w_f_6_3, strides=[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>]))</span><br><span class="line">        w_f_6_4 = weight_variable([<span class="number">3</span>, <span class="number">3</span>, <span class="number">128</span>, <span class="number">128</span>], name=<span class="string">"w_f_6_4"</span>)</span><br><span class="line">        conv_f_6_4 = tf.nn.relu(conv2d(conv_f_6_3, w_f_6_4, strides=[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>]))</span><br><span class="line">        w_f_6_5 = weight_variable([<span class="number">3</span>, <span class="number">3</span>, <span class="number">128</span>, <span class="number">64</span>], name=<span class="string">"w_f_6_5"</span>)</span><br><span class="line">        conv_f_6_5 = tf.nn.relu(conv2d(conv_f_6_4, w_f_6_5, strides=[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>]))</span><br><span class="line">        w_f_6_6 = weight_variable([<span class="number">3</span>, <span class="number">3</span>, <span class="number">64</span>, <span class="number">64</span>], name=<span class="string">"w_f_6_6"</span>)</span><br><span class="line">        conv_f_6_6 = tf.nn.relu(conv2d(conv_f_6_5, w_f_6_6, strides=[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>]))</span><br><span class="line">        w_f_6_7 = weight_variable([<span class="number">3</span>, <span class="number">3</span>, <span class="number">64</span>, <span class="number">3</span>], name=<span class="string">"w_f_6_7"</span>)</span><br><span class="line">        enhanced = tf.nn.tanh(conv2d(conv_f_6_6, w_f_6_7, strides=[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>]))</span><br><span class="line">    <span class="keyword">return</span> enhanced</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">weight_variable</span><span class="params">(shape, name)</span>:</span></span><br><span class="line">    initial = tf.truncated_normal_initializer(stddev=<span class="number">0.01</span>)</span><br><span class="line">    <span class="keyword">return</span> tf.get_variable(name=name, shape=shape, initializer=initial)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">bias_variable</span><span class="params">(shape, name)</span>:</span></span><br><span class="line">    initial = tf.constant_initializer(<span class="number">0.01</span>)</span><br><span class="line">    <span class="keyword">return</span> tf.get_variable(name=name, shape=shape, initializer=initial)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">conv2d</span><span class="params">(x, W, strides=[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>], padding=<span class="string">'SAME'</span>)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> tf.nn.conv2d(x, W, strides=strides, padding=padding)</span><br><span class="line">weight_init = tf.random_normal_initializer(mean=<span class="number">0.0</span>, stddev=<span class="number">0.02</span>)</span><br><span class="line">weight_regularizer = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">deconv2d</span><span class="params">(batch_input, out_channels, kernel_size)</span>:</span></span><br><span class="line">    <span class="comment"># [batch, in_height, in_width, in_channels] =&gt; [batch, out_height, out_width, out_channels]</span></span><br><span class="line">    initializer = tf.random_normal_initializer(<span class="number">0</span>, <span class="number">0.02</span>)</span><br><span class="line">    <span class="keyword">return</span> tf.layers.conv2d_transpose(batch_input, out_channels, kernel_size=kernel_size, strides=(<span class="number">2</span>, <span class="number">2</span>), padding=<span class="string">"same"</span>, kernel_initializer=initializer)</span><br></pre></td></tr></table></figure>

<p>不要纠结于结构复用，我在实现的时候单纯为了顺着他的网络复现的。</p>
]]></content>
      <categories>
        <category>论文复现</category>
        <category>tensorflow</category>
      </categories>
      <tags>
        <tag>论文复现</tag>
      </tags>
  </entry>
  <entry>
    <title>Keras函数（一）</title>
    <url>/2020/02/15/Keras%E5%87%BD%E6%95%B0%EF%BC%88%E4%B8%80%EF%BC%89/</url>
    <content><![CDATA[<p>Lambda层 || reverse函数<a id="more"></a></p>
<h3 id="Lambda层"><a href="#Lambda层" class="headerlink" title="Lambda层"></a>Lambda层</h3><p>知道lambda层首先需要知道一个函数lambda函数，lambda函数又称匿名函数。如果你要实现的业务逻辑很简单，比如说简单的a+b，那就无需定义函数名。如下例子</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">lambda</span> x, y: x+y</span><br></pre></td></tr></table></figure>

<p>x和y表示函数的参数，冒号后面的x+y则是函数的业务逻辑，也是函数的输出。类似于如上简单的加法，你一眼就能看得出来，因此我们可以使用lambda函数定义。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>add = <span class="keyword">lambda</span> x, y : x+y</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>add</span><br><span class="line">&lt;function &lt;<span class="keyword">lambda</span>&gt; at <span class="number">0x102bc2140</span>&gt;</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>add(<span class="number">1</span>,<span class="number">2</span>)</span><br><span class="line"><span class="number">3</span></span><br></pre></td></tr></table></figure>

<p>如果你只是想对流经该层的数据做个变换，而这个变换本身没有什么需要学习的参数，那么直接用Lambda层是最合适的了。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">keras.layers.core.Lambda(function, output_shape=<span class="literal">None</span>, mask=<span class="literal">None</span>, arguments=<span class="literal">None</span>)</span><br></pre></td></tr></table></figure>

<p>function：要实现的函数，该函数仅接受一个变量，即上一层的输出</p>
<p>output_shape：函数应该返回的值的shape，可以是一个tuple，也可以是一个根据输入shape计算输出shape的函数</p>
<p>mask: 掩膜</p>
<p>arguments：可选，字典，用来记录向函数中传递的其他关键字参数</p>
<h3 id="reverse函数"><a href="#reverse函数" class="headerlink" title="reverse函数"></a>reverse函数</h3><p>tf.keras.backend.reverse(x, axes) 沿指定轴反转张量</p>
<p>x: 要反转的张量</p>
<p>axes：整数或整数迭代。要反转的轴</p>
]]></content>
      <categories>
        <category>深度学习</category>
        <category>keras</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title>hexo + NexT搭建个人主页（一）</title>
    <url>/2020/02/14/hexo-NexT%E4%B8%BB%E9%A2%98%E6%90%AD%E5%BB%BA%E7%BE%8E%E8%A7%82%E7%9A%84%E4%B8%AA%E4%BA%BA%E4%B8%BB%E9%A1%B5%EF%BC%88%E4%B8%80%EF%BC%89/</url>
    <content><![CDATA[<p>本篇将使用hexo工具加Next主题初步搭建一个blog，hexo能极大的加速你的blog的搭建，无需HTML，CSS，JS等基础知识，上手迅速。同时，当你使用过NexT主题之后，你一定会深深地爱上她。<a id="more"></a></p>
<h3 id="安装工具"><a href="#安装工具" class="headerlink" title="安装工具"></a>安装工具</h3><ul>
<li>Github</li>
<li>Node.Js</li>
</ul>
<h4 id="Github"><a href="#Github" class="headerlink" title="Github"></a>Github</h4><p>Github安装以及如何配置最基本的个人主页已经在上一节进行过介绍了，不明白的可以去看上一篇。</p>
<h4 id="Node-Js"><a href="#Node-Js" class="headerlink" title="Node.Js"></a>Node.Js</h4><p><a href="https://nodejs.org/en/" target="_blank" rel="noopener">Node.Js</a>是一个工具，当你觉得等待后端是一个繁琐耗时的过程时，你可以使用node自个儿解决问题，在我们这里他很简单，就是npm，npm就是统一维护升级你的依赖包，使你管理文件的时候显得不那么手忙脚乱。</p>
<p><img src="https://i.loli.net/2020/02/15/38e1UpC2xGkYst6.png" alt="image-20200214173926555.png"></p>
<p>安装完之后，使用npm -v 查看环境变量是否配置完成。如果没有成功，需要自己手动配置环境变量。</p>
<hr>
<h3 id="安装Hexo"><a href="#安装Hexo" class="headerlink" title="安装Hexo"></a>安装Hexo</h3><p>Node.Js安装好之后，咱就可以正式开始搭建属于自己的博客。打开cmd（命令行），输入</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">npm install -g hexo-cli</span><br></pre></td></tr></table></figure>

<p>随便在哪里新建文件夹，文件夹的名字任选，我就取名为blog，在cmd中切换到blog文件夹，输入命令</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hexo init</span><br></pre></td></tr></table></figure>

<p>安装成功，此时blog的文件结构应该像下图</p>
<p><img src="https://i.loli.net/2020/02/15/wUohBkQAafiDc2j.png" alt="image-20200214213146241.png"></p>
<p><img src="https://i.loli.net/2020/02/15/dPNAaubxXKsBErW.png" alt="image-20200214213348369.png"></p>
<p>输入命令</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hexo server [-p](p是开放端口，默认4000，但当4000端口被其他进程占据，可以用p指定其他端口)</span><br></pre></td></tr></table></figure>

<p>测试，在网址中输入localhost:4000如果能跳转到如下界面，证明你的hexo已经配置完成。</p>
<p><img src="https://i.loli.net/2020/02/15/pYJGLS8ksCwVjFr.png" alt="image-20200214213722822.png"></p>
<hr>
<h3 id="配置SSH-keys"><a href="#配置SSH-keys" class="headerlink" title="配置SSH keys"></a>配置SSH keys</h3><p>右键桌面，选择Git Bash，配置个人信息，输入命令，yourname是你的github账号的用户名，yourEmail是注册时用的邮箱 。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">git config --global user.name &quot;yourname&quot;</span><br><span class="line">git config --global user.email &quot;yourEmail&quot;</span><br><span class="line">ssh-keygen -t rsa -C &quot;yourEmail&quot;</span><br></pre></td></tr></table></figure>

<p>在秘钥生成后，会有对应的存放文件地址，搜索.ssh文件夹，去该地址中，找到<code>id_rsa.pub</code>文件，复制里面的内容，粘贴至GitHub中，点击右上角用户头像下的<code>Settings</code>，之后点击左侧的<code>SSH and GPG keys</code>，找到<code>New SSH key</code>点击，输入<code>title</code>，并将之前复制的内容粘贴到下面的<code>key</code>中，最后点击<code>Add SSH key</code>，完成：</p>
<p><img src="https://i.loli.net/2020/02/15/W8DGqFYm6o3xfQi.png" alt="image-20200215000748789.png">博客根目录下寻找_config.yml，将修改deploy</p>
<figure class="highlight yml"><table><tr><td class="code"><pre><span class="line"><span class="attr">deploy:</span></span><br><span class="line">  <span class="attr">type:</span> <span class="string">git</span></span><br><span class="line">  <span class="attr">repo:</span> <span class="string">https://github.com/yourname/yourname.github.io.git</span></span><br><span class="line">  <span class="attr">branch:</span> <span class="string">master</span></span><br></pre></td></tr></table></figure>

<p>在blog文件下使用命令，将blog打包部署到github</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hexo d -g</span><br></pre></td></tr></table></figure>

<hr>
<h3 id="安装NexT主题"><a href="#安装NexT主题" class="headerlink" title="安装NexT主题"></a>安装NexT主题</h3><p>默认的主题是landscape，有一说一，确实不太好看，因此我们选择hexo官方推荐的NexT主题。在github搜索栏中搜索hexo-next。下面一个是NexT6，仍有团队在维护，可以看更新时间，上面的已经不维护更新了，推荐NexT6. </p>
<p><img src="https://i.loli.net/2020/02/15/4ugh1W3P6lZnEzd.png" alt="image-20200215001112500.png"></p>
<p>选择git clone或者直接下载压缩包。将next文件夹放到/blog/themes下，里面其实有一个landscape。OK，在站点根目录下有一个_config.yml，搜索theme，然后进行修改</p>
<figure class="highlight yml"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Extensions</span></span><br><span class="line"><span class="comment">## Plugins: https://hexo.io/plugins/</span></span><br><span class="line"><span class="comment">## Themes: https://hexo.io/themes/</span></span><br><span class="line"><span class="attr">theme:</span> <span class="string">next</span> <span class="string">(跟你的next文件夹名字一样)</span></span><br></pre></td></tr></table></figure>

<p>此时，输入命令，完成</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hexo server</span><br></pre></td></tr></table></figure>

<p><img src="https://i.loli.net/2020/02/15/3Vvj6ouAimt1HOa.png" alt="image-20200215004304632.png"></p>
<hr>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">npm install -g hexo-cli 安装hexo</span><br><span class="line">hexo init 建立站点文件夹</span><br><span class="line">hexo server [p] 在本地进行演示</span><br><span class="line">hexo generate 生成public文件夹</span><br><span class="line">hexo deploy 将博客打包上传到github</span><br><span class="line">hexo clean 清除本地缓存文件</span><br></pre></td></tr></table></figure>

]]></content>
      <categories>
        <category>互联网</category>
      </categories>
      <tags>
        <tag>互联网</tag>
      </tags>
  </entry>
  <entry>
    <title>如何搭建github-page</title>
    <url>/2020/02/09/%E5%A6%82%E4%BD%95%E6%90%AD%E5%BB%BAgithub-page/</url>
    <content><![CDATA[<p>最近有想法搭建自己的blog，主要用来记录自己的学习，又苦于没有服务器，因此选择在Github上搭建自己博客。看了半天教程，踩了不少坑，总结一套比较适用新手的方法。（纯小白）<a id="more"></a></p>
<h4 id="准备"><a href="#准备" class="headerlink" title="准备"></a>准备</h4><ul>
<li>Git账号注册</li>
<li>新建仓库</li>
<li>仓库属性配置</li>
<li>访问方式</li>
</ul>
<h5 id="git下载"><a href="#git下载" class="headerlink" title="git下载"></a>git下载</h5><p>首先需要一个Github账号，<a href="https://github.com/" target="_blank" rel="noopener">github官网</a>注册属于自己的Github账号。GitHub作为全球最大的程序员交友网站，是一个代码托管平台和开发者社区，开发者可以在Github上创建自己的开源项目并与其他开发者协作编码，有了Github账号，我们就可以在clone的道路上越走越远。</p>
<p><img src="https://i.loli.net/2020/02/09/LkcOu924BrXonGy.png" alt="image-20200209142944276.png"></p>
<hr>
<h5 id="新建仓库"><a href="#新建仓库" class="headerlink" title="新建仓库"></a>新建仓库</h5><ol>
<li>首先新建一个Repositories（项目），</li>
</ol>
<p><img src="https://i.loli.net/2020/02/09/QrCS3jODFsTcot5.png" alt="image-20200209143606868.png"></p>
<ol start="2">
<li>千万注意项目名称命名为<strong>你的名字.github.io</strong>，然后点击create </li>
</ol>
<p><img src="https://i.loli.net/2020/02/09/Vf7ZOPeguWLdXx8.png" alt="image-20200209145106208.png"></p>
<hr>
<h5 id="仓库属性配置"><a href="#仓库属性配置" class="headerlink" title="仓库属性配置"></a>仓库属性配置</h5><ol>
<li>进入settings，确保名字是<strong>* Git_id.github.io *</strong></li>
</ol>
<p><img src="https://i.loli.net/2020/02/09/s9KlVN2OGbrfRce.png" alt="image-20200209145305495.png"></p>
<ol start="2">
<li>然后往下拉找到<strong><em>GitHub Pages</em></strong></li>
</ol>
<p><img src="https://i.loli.net/2020/02/09/cspR46TCamArzfU.png" alt="image-20200209145747750.png"></p>
<ol start="3">
<li>选择theme</li>
</ol>
<p><img src="https://i.loli.net/2020/02/09/tdkMXxLgNb7YZKH.png" alt="image-20200209145929455.png"></p>
<ol start="4">
<li>填上描述，commit修改即可（可以不填）</li>
</ol>
<p><img src="https://i.loli.net/2020/02/09/IMfNCEZsgAcWPj2.png" alt="image-20200209150112237.png"></p>
<hr>
<h5 id="访问方式"><a href="#访问方式" class="headerlink" title="访问方式"></a>访问方式</h5><p><img src="https://i.loli.net/2020/02/09/dKkDH3PjeGc7nrp.png" alt="image-20200209150251311.png"></p>
<hr>
<h5 id="配置成功"><a href="#配置成功" class="headerlink" title="配置成功"></a>配置成功</h5><p><img src="https://i.loli.net/2020/02/09/cO8EtNFH2mTeRuA.png" alt="image-20200209174132616.png"></p>
]]></content>
      <categories>
        <category>互联网</category>
      </categories>
      <tags>
        <tag>互联网</tag>
      </tags>
  </entry>
</search>
